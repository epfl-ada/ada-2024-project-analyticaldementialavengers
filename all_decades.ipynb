{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used in LDA\n",
    "\n",
    "def text_split_new(text) :\n",
    "\n",
    "    #Initialize a set of frequent and not relevent words in english\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #Remove from text all punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "\n",
    "    #Tokenise the text in words and convert in small letters\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    #Filter the words depending on their grammatical class\n",
    "    \n",
    "    text_splited = []\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        #Find the grammatical class (pos) of a word and tag it with it\n",
    "        pos = pos_tag([word])[0][1]\n",
    "        #Keep only the nouns (singular and plural)\n",
    "        if pos in ['NN', 'NNS']: #and word not in stop_words:\n",
    "            text_splited.append(word)\n",
    "    \n",
    "    return text_splited\n",
    "\n",
    "def bag_of_words(text_splited) : \n",
    "    \n",
    "    #Create a dictionary mapping each word to a unique id\n",
    "    dictionary = Dictionary([text_splited])\n",
    "    \n",
    "    \n",
    "    corpus = dictionary.doc2bow(text_splited)\n",
    "    \n",
    "    return corpus, dictionary\n",
    "\n",
    "def lda(df_line):\n",
    "    corpus, dictionary = df_line  # Unpacking the tuple directly\n",
    "    lda_model = LdaModel([corpus], num_topics=1, id2word=dictionary)\n",
    "    return lda_model\n",
    "\n",
    "def get_topics(lda_model):\n",
    "    \n",
    "    return lda_model.show_topics(num_topics=1, num_words=50, formatted=True)\n",
    "\n",
    "def extract(topics_tuple):\n",
    "    topics = []\n",
    "    pattern = re.compile(r'(\\d+\\.\\d+)\\*\"(.*?)\"')\n",
    "    for topic_id, topics_str in topics_tuple:\n",
    "        matches = pattern.findall(topics_str)\n",
    "        for match in matches:\n",
    "            freq = float(match[0])\n",
    "            word = match[1]\n",
    "            topics.append((word, freq))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words for the decade 1900s:  [(0, '0.006*\"World\" + 0.005*\"Machine\" + 0.005*\"Wayback\" + 0.005*\"Population\" + 0.004*\"History\" + 0.003*\"ISBN\" + 0.003*\"Data\" + 0.003*\"estimate\" + 0.003*\"flight\" + 0.003*\"Sheet\" + 0.003*\"Haub\" + 0.003*\"machine\" + 0.002*\"Empire\" + 0.002*\"States\" + 0.002*\"engine\" + 0.002*\"Fessenden\" + 0.002*\"US\" + 0.002*\"m\" + 0.002*\"pp\" + 0.002*\"de\" + 0.002*\"p\" + 0.002*\"Company\" + 0.002*\"Edison\" + 0.002*\"air\" + 0.002*\"Years\" + 0.002*\"decade\" + 0.002*\"Panama\" + 0.002*\"html\" + 0.002*\"http\" + 0.002*\"device\" + 0.002*\"Popular\" + 0.002*\"Time\" + 0.001*\"time\" + 0.001*\"developed\" + 0.001*\"Worlds\" + 0.001*\"earthquake\" + 0.001*\"car\" + 0.001*\"population\" + 0.001*\"httpsAvebarchive\" + 0.001*\"production\" + 0.001*\"paper\" + 0.001*\"Great\" + 0.001*\"Vol\" + 0.001*\"Radio\" + 0.001*\"radio\" + 0.001*\"estimates\" + 0.001*\"phonograph\" + 0.001*\"flights\" + 0.001*\"years\" + 0.001*\"built\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1910s:  [(0, '0.009*\"World\" + 0.006*\"Revolution\" + 0.004*\"baseball\" + 0.004*\"people\" + 0.004*\"States\" + 0.004*\"player\" + 0.003*\"decade\" + 0.003*\"Russia\" + 0.003*\"revolution\" + 0.003*\"Army\" + 0.003*\"Henry\" + 0.003*\"movements\" + 0.003*\"events\" + 0.003*\"Empire\" + 0.003*\"establishment\" + 0.003*\"Ottoman\" + 0.003*\"years\" + 0.002*\"states\" + 0.002*\"death\" + 0.002*\"Austria\" + 0.002*\"RMS\" + 0.002*\"Samuel\" + 0.002*\"p\" + 0.002*\"JSTOR\" + 0.002*\"Franz\" + 0.002*\"film\" + 0.002*\"decades\" + 0.002*\"minutes\" + 0.002*\"monarchy\" + 0.002*\"Williams\" + 0.002*\"Eddie\" + 0.002*\"Ferdinand\" + 0.002*\"Archduke\" + 0.002*\"online\" + 0.002*\"Era\" + 0.002*\"ISBN\" + 0.002*\"Constitution\" + 0.002*\"pp\" + 0.002*\"Titanic\" + 0.002*\"US\" + 0.002*\"Art\" + 0.002*\"art\" + 0.002*\"empires\" + 0.002*\"V\" + 0.002*\"Pickford\" + 0.002*\"T\" + 0.002*\"D\" + 0.002*\"Arab\" + 0.002*\"war\" + 0.002*\"Cubism\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1920s:  [(0, '0.008*\"p\" + 0.005*\"rentals\" + 0.005*\"film\" + 0.005*\"States\" + 0.005*\"ISBN\" + 0.004*\"decade\" + 0.004*\"Civil\" + 0.003*\"deaths\" + 0.003*\"worldwide\" + 0.003*\"women\" + 0.003*\"History\" + 0.003*\"Press\" + 0.003*\"profit\" + 0.003*\"history\" + 0.003*\"cost\" + 0.003*\"Cost\" + 0.003*\"movie\" + 0.003*\"World\" + 0.003*\"forces\" + 0.003*\"earthquake\" + 0.003*\"US\" + 0.003*\"radio\" + 0.003*\"Twenties\" + 0.002*\"suffrage\" + 0.002*\"world\" + 0.002*\"Independence\" + 0.002*\"war\" + 0.002*\"Great\" + 0.002*\"silent\" + 0.002*\"Block\" + 0.002*\"magnitude\" + 0.002*\"people\" + 0.002*\"Jazz\" + 0.002*\"State\" + 0.002*\"Prohibition\" + 0.002*\"Hall\" + 0.002*\"Company\" + 0.002*\"Fascist\" + 0.002*\"ch\" + 0.002*\"years\" + 0.002*\"Fool\" + 0.002*\"films\" + 0.002*\"Ireland\" + 0.002*\"members\" + 0.002*\"invents\" + 0.002*\"hit\" + 0.002*\"Benito\" + 0.002*\"Treaty\" + 0.002*\"Price\" + 0.002*\"Socialist\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1930s:  [(0, '0.007*\"p\" + 0.005*\"Great\" + 0.005*\"Depression\" + 0.005*\"World\" + 0.005*\"film\" + 0.004*\"boxoffice\" + 0.004*\"States\" + 0.004*\"Cost\" + 0.004*\"ISBN\" + 0.003*\"rentals\" + 0.003*\"II\" + 0.003*\"decade\" + 0.003*\"Nazi\" + 0.003*\"Hitler\" + 0.003*\"cost\" + 0.003*\"worldwide\" + 0.003*\"Press\" + 0.003*\"Block\" + 0.003*\"world\" + 0.002*\"History\" + 0.002*\"Snow\" + 0.002*\"Empire\" + 0.002*\"countries\" + 0.002*\"people\" + 0.002*\"US\" + 0.002*\"Dust\" + 0.002*\"Bowl\" + 0.002*\"years\" + 0.002*\"State\" + 0.002*\"release\" + 0.002*\"Wilson\" + 0.002*\"Civil\" + 0.002*\"deaths\" + 0.002*\"Hughes\" + 0.002*\"Austria\" + 0.002*\"profit\" + 0.002*\"fought\" + 0.002*\"Thirties\" + 0.002*\"territory\" + 0.002*\"forces\" + 0.002*\"Profit\" + 0.002*\"events\" + 0.002*\"production\" + 0.002*\"Film\" + 0.002*\"history\" + 0.002*\"state\" + 0.002*\"war\" + 0.002*\"Gone\" + 0.001*\"pp\" + 0.001*\"Foreign\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1950s:  [(0, '0.010*\"1950s\" + 0.007*\"States\" + 0.005*\"war\" + 0.005*\"Revolution\" + 0.004*\"History\" + 0.004*\"decade\" + 0.004*\"people\" + 0.003*\"World\" + 0.003*\"independence\" + 0.003*\"coup\" + 0.003*\"US\" + 0.003*\"death\" + 0.003*\"Organization\" + 0.003*\"leader\" + 0.003*\"government\" + 0.003*\"North\" + 0.003*\"countries\" + 0.003*\"transistor\" + 0.002*\"power\" + 0.002*\"communist\" + 0.002*\"Suez\" + 0.002*\"détat\" + 0.002*\"p\" + 0.002*\"satellite\" + 0.002*\"ISBN\" + 0.002*\"Greenland\" + 0.002*\"world\" + 0.002*\"polio\" + 0.002*\"shot\" + 0.002*\"Truman\" + 0.002*\"Life\" + 0.002*\"nations\" + 0.002*\"II\" + 0.002*\"Sputnik\" + 0.002*\"J\" + 0.002*\"S\" + 0.002*\"crashed\" + 0.002*\"states\" + 0.002*\"inflation\" + 0.002*\"vaccine\" + 0.002*\"1940s\" + 0.002*\"Nasser\" + 0.002*\"Crisis\" + 0.002*\"Cuban\" + 0.001*\"months\" + 0.001*\"Abdel\" + 0.001*\"Labs\" + 0.001*\"regime\" + 0.001*\"army\" + 0.001*\"board\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1960s:  [(0, '0.009*\"1960s\" + 0.008*\"movement\" + 0.007*\"States\" + 0.006*\"US\" + 0.004*\"Sixties\" + 0.004*\"decade\" + 0.004*\"rights\" + 0.003*\"History\" + 0.003*\"end\" + 0.003*\"people\" + 0.003*\"government\" + 0.003*\"Kennedy\" + 0.003*\"president\" + 0.003*\"Revolution\" + 0.003*\"time\" + 0.002*\"coup\" + 0.002*\"protests\" + 0.002*\"ISBN\" + 0.002*\"death\" + 0.002*\"war\" + 0.002*\"Mao\" + 0.002*\"Press\" + 0.002*\"Civil\" + 0.002*\"mass\" + 0.002*\"world\" + 0.002*\"countries\" + 0.002*\"country\" + 0.002*\"F\" + 0.002*\"East\" + 0.002*\"leader\" + 0.002*\"revolution\" + 0.002*\"music\" + 0.002*\"movements\" + 0.001*\"pp\" + 0.001*\"independence\" + 0.001*\"Cuban\" + 0.001*\"World\" + 0.001*\"growth\" + 0.001*\"discrimination\" + 0.001*\"Rights\" + 0.001*\"earthquake\" + 0.001*\"groups\" + 0.001*\"Apollo\" + 0.001*\"power\" + 0.001*\"Communist\" + 0.001*\"Times\" + 0.001*\"Americans\" + 0.001*\"Great\" + 0.001*\"mission\" + 0.001*\"left\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1970s:  [(0, '0.011*\"1970s\" + 0.006*\"decade\" + 0.005*\"States\" + 0.004*\"US\" + 0.004*\"people\" + 0.004*\"world\" + 0.003*\"movement\" + 0.003*\"power\" + 0.003*\"government\" + 0.003*\"oil\" + 0.003*\"women\" + 0.003*\"crisis\" + 0.003*\"countries\" + 0.003*\"East\" + 0.003*\"ISBN\" + 0.003*\"end\" + 0.002*\"war\" + 0.002*\"years\" + 0.002*\"video\" + 0.002*\"woman\" + 0.002*\"game\" + 0.002*\"worlds\" + 0.002*\"p\" + 0.002*\"coup\" + 0.002*\"Revolution\" + 0.002*\"Pakistan\" + 0.002*\"1960s\" + 0.002*\"II\" + 0.002*\"pp\" + 0.002*\"country\" + 0.002*\"cars\" + 0.002*\"crashed\" + 0.002*\"Civil\" + 0.002*\"saw\" + 0.002*\"home\" + 0.002*\"Flight\" + 0.002*\"year\" + 0.002*\"Seventies\" + 0.002*\"History\" + 0.002*\"excerpt\" + 0.001*\"Quebec\" + 0.001*\"history\" + 0.001*\"time\" + 0.001*\"cyclone\" + 0.001*\"include\" + 0.001*\"market\" + 0.001*\"state\" + 0.001*\"Liberation\" + 0.001*\"space\" + 0.001*\"Peoples\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1980s:  [(0, '0.006*\"decade\" + 0.005*\"people\" + 0.005*\"US\" + 0.005*\"government\" + 0.004*\"end\" + 0.003*\"States\" + 0.003*\"Flight\" + 0.003*\"market\" + 0.003*\"video\" + 0.003*\"world\" + 0.003*\"computer\" + 0.002*\"time\" + 0.002*\"years\" + 0.002*\"Civil\" + 0.002*\"game\" + 0.002*\"World\" + 0.002*\"ISBN\" + 0.002*\"countries\" + 0.002*\"p\" + 0.002*\"System\" + 0.002*\"Times\" + 0.002*\"systems\" + 0.002*\"North\" + 0.002*\"Nintendo\" + 0.002*\"communist\" + 0.002*\"president\" + 0.002*\"computers\" + 0.002*\"board\" + 0.002*\"leader\" + 0.002*\"become\" + 0.002*\"country\" + 0.002*\"Iran\" + 0.002*\"games\" + 0.002*\"system\" + 0.002*\"II\" + 0.002*\"Cold\" + 0.001*\"attack\" + 0.001*\"include\" + 0.001*\"protests\" + 0.001*\"music\" + 0.001*\"reforms\" + 0.001*\"Space\" + 0.001*\"war\" + 0.001*\"shot\" + 0.001*\"industry\" + 0.001*\"members\" + 0.001*\"disaster\" + 0.001*\"use\" + 0.001*\"forces\" + 0.001*\"strike\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1990s:  [(0, '0.005*\"States\" + 0.004*\"decade\" + 0.004*\"people\" + 0.003*\"game\" + 0.003*\"World\" + 0.003*\"end\" + 0.003*\"countries\" + 0.003*\"video\" + 0.003*\"series\" + 0.003*\"years\" + 0.002*\"Picture\" + 0.002*\"Academy\" + 0.002*\"Sciences\" + 0.002*\"US\" + 0.002*\"Arts\" + 0.002*\"Motion\" + 0.002*\"forces\" + 0.002*\"bombing\" + 0.002*\"Clinton\" + 0.002*\"games\" + 0.002*\"Super\" + 0.002*\"Times\" + 0.002*\"world\" + 0.002*\"time\" + 0.002*\"North\" + 0.002*\"market\" + 0.002*\"Mario\" + 0.002*\"independence\" + 0.002*\"popularity\" + 0.002*\"SSR\" + 0.002*\"genocide\" + 0.002*\"Ireland\" + 0.002*\"music\" + 0.002*\"decades\" + 0.002*\"conflict\" + 0.002*\"bestselling\" + 0.002*\"Civil\" + 0.002*\"becomes\" + 0.002*\"media\" + 0.002*\"ISBN\" + 0.002*\"government\" + 0.002*\"Pokémon\" + 0.001*\"Yeltsin\" + 0.001*\"women\" + 0.001*\"crashed\" + 0.001*\"Agreement\" + 0.001*\"country\" + 0.001*\"history\" + 0.001*\"East\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2000s:  [(0, '0.005*\"decade\" + 0.004*\"Machine\" + 0.004*\"Wayback\" + 0.004*\"people\" + 0.003*\"US\" + 0.003*\"World\" + 0.003*\"States\" + 0.003*\"News\" + 0.003*\"world\" + 0.002*\"countries\" + 0.002*\"PDF\" + 0.002*\"BBC\" + 0.002*\"conflict\" + 0.002*\"http\" + 0.002*\"Times\" + 0.002*\"games\" + 0.001*\"years\" + 0.001*\"Press\" + 0.001*\"game\" + 0.001*\"Population\" + 0.001*\"video\" + 0.001*\"attacks\" + 0.001*\"war\" + 0.001*\"end\" + 0.001*\"popularity\" + 0.001*\"Data\" + 0.001*\"government\" + 0.001*\"crisis\" + 0.001*\"de\" + 0.001*\"become\" + 0.001*\"time\" + 0.001*\"PlayStation\" + 0.001*\"UK\" + 0.001*\"Internet\" + 0.001*\"Civil\" + 0.001*\"population\" + 0.001*\"use\" + 0.001*\"groups\" + 0.001*\"https\" + 0.001*\"htt\" + 0.001*\"estimate\" + 0.001*\"rise\" + 0.001*\"earthquake\" + 0.001*\"Iraq\" + 0.001*\"httpswebarchiveorg\" + 0.001*\"media\" + 0.001*\"history\" + 0.001*\"television\" + 0.001*\"Philippines\" + 0.001*\"bestselling\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2010s:  [(0, '0.006*\"News\" + 0.005*\"States\" + 0.004*\"BBC\" + 0.004*\"people\" + 0.004*\"election\" + 0.003*\"decade\" + 0.003*\"US\" + 0.003*\"Times\" + 0.003*\"protests\" + 0.003*\"earthquake\" + 0.002*\"president\" + 0.002*\"attack\" + 0.002*\"government\" + 0.002*\"Flight\" + 0.002*\"games\" + 0.002*\"fire\" + 0.002*\"Philippines\" + 0.002*\"Russia\" + 0.002*\"world\" + 0.002*\"history\" + 0.002*\"Arab\" + 0.002*\"war\" + 0.002*\"North\" + 0.002*\"time\" + 0.002*\"PDF\" + 0.002*\"Reuters\" + 0.001*\"Trump\" + 0.001*\"ISSN\" + 0.001*\"crisis\" + 0.001*\"Pakistan\" + 0.001*\"Wii\" + 0.001*\"http\" + 0.001*\"Iraq\" + 0.001*\"Death\" + 0.001*\"CNN\" + 0.001*\"Nintendo\" + 0.001*\"countries\" + 0.001*\"damage\" + 0.001*\"saw\" + 0.001*\"Event\" + 0.001*\"de\" + 0.001*\"Date\" + 0.001*\"toll\" + 0.001*\"Country\" + 0.001*\"Typhoon\" + 0.001*\"Description\" + 0.001*\"Al\" + 0.001*\"country\" + 0.001*\"attacks\" + 0.001*\"Sudan\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#First for loop to get the most used words for each decade\n",
    "\n",
    "paths= ['data/wikipedia_timeline/1900s.txt', 'data/wikipedia_timeline/1910s.txt', 'data/wikipedia_timeline/1920s.txt', 'data/wikipedia_timeline/1930s.txt', 'data/wikipedia_timeline/1950s.txt', 'data/wikipedia_timeline/1960s.txt', 'data/wikipedia_timeline/1970s.txt', 'data/wikipedia_timeline/1980s.txt', 'data/wikipedia_timeline/1990s.txt', 'data/wikipedia_timeline/2000s.txt', 'data/wikipedia_timeline/2010s.txt']\n",
    "\n",
    "\n",
    "for path in paths:\n",
    "    decade_name = path.split('/')[-1].split('.')[0]\n",
    "    decade=pd.read_csv(path, delimiter='\\t', header=None)\n",
    "    decade=decade.to_string(index=False, header=False)\n",
    "    splitted_decade=text_split_new(decade)\n",
    "    bag_decade=bag_of_words(splitted_decade)\n",
    "    lda_decade=lda(bag_decade)\n",
    "    print(f\"Most used words for the decade {decade_name}: \",get_topics(lda_decade))\n",
    "    print(\"the type of frequencies \", type(lda_decade.get_topics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       World  Machine  Wayback  Population  History   ISBN   Data  estimate  \\\n",
      "1900s  0.006    0.005    0.005       0.005    0.004  0.003  0.003     0.003   \n",
      "1910s  0.009    0.000    0.000       0.000    0.000  0.002  0.000     0.000   \n",
      "1920s  0.003    0.000    0.000       0.000    0.003  0.005  0.000     0.000   \n",
      "1930s  0.005    0.000    0.000       0.000    0.002  0.004  0.000     0.000   \n",
      "1950s  0.003    0.000    0.000       0.000    0.004  0.002  0.000     0.000   \n",
      "\n",
      "       flight  Sheet  ...  damage  Event  toll  Country  Date  Description  \\\n",
      "1900s   0.003  0.003  ...     0.0    0.0   0.0      0.0   0.0          0.0   \n",
      "1910s   0.000  0.000  ...     0.0    0.0   0.0      0.0   0.0          0.0   \n",
      "1920s   0.000  0.000  ...     0.0    0.0   0.0      0.0   0.0          0.0   \n",
      "1930s   0.000  0.000  ...     0.0    0.0   0.0      0.0   0.0          0.0   \n",
      "1950s   0.000  0.000  ...     0.0    0.0   0.0      0.0   0.0          0.0   \n",
      "\n",
      "        Al  Typhoon  Sudan  Syria  \n",
      "1900s  0.0      0.0    0.0    0.0  \n",
      "1910s  0.0      0.0    0.0    0.0  \n",
      "1920s  0.0      0.0    0.0    0.0  \n",
      "1930s  0.0      0.0    0.0    0.0  \n",
      "1950s  0.0      0.0    0.0    0.0  \n",
      "\n",
      "[5 rows x 296 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store word frequencies for each decade\n",
    "word_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for path in paths:\n",
    "    # Extract the decade from the file path\n",
    "    decade_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # Read the file\n",
    "    decade = pd.read_csv(path, delimiter='\\t', header=None)\n",
    "    decade = decade.to_string(index=False, header=False)\n",
    "    \n",
    "    # Process the text\n",
    "    splitted_decade = text_split_new(decade)\n",
    "    bag_decade = bag_of_words(splitted_decade)\n",
    "    lda_decade = lda(bag_decade)\n",
    "    \n",
    "    # Get the topics and their frequencies\n",
    "    topics_str = get_topics(lda_decade)\n",
    "    # extract() function was written to extract the topics and their frequencies given their type\n",
    "    topics = extract(topics_str)\n",
    "    \n",
    "    #print(f\"Most used words for the decade {decade_name}: \", topics)\n",
    "    # Update the word frequencies for each decade\n",
    "    for word, freq in topics:\n",
    "        word_frequencies[decade_name][word] = freq\n",
    "        #print(word_frequencies)\n",
    "\n",
    "# Convert the word frequencies dictionary to a DataFrame\n",
    "df_word_frequencies = pd.DataFrame(word_frequencies).fillna(0)\n",
    "\n",
    "# Transpose the DataFrame to have decades as rows and words as columns\n",
    "df_word_frequencies = df_word_frequencies.T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_word_frequencies.head())\n",
    "\n",
    "#save in csv file\n",
    "df_word_frequencies.to_csv(\"word_frequencies.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used in LDA\n",
    "\n",
    "def text_split_new(text) :\n",
    "\n",
    "    #Initialize a set of frequent and not relevent words in english\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #Remove from text all punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "\n",
    "    #Tokenise the text in words and convert in small letters\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    #Filter the words depending on their grammatical class\n",
    "    \n",
    "    text_splited = []\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        #Find the grammatical class (pos) of a word and tag it with it\n",
    "        pos = pos_tag([word])[0][1]\n",
    "        #Keep only the nouns (singular and plural)\n",
    "        if pos in ['NN', 'NNS']: #and word not in stop_words:\n",
    "            text_splited.append(word)\n",
    "    \n",
    "    return text_splited\n",
    "\n",
    "def bag_of_words(text_splited) : \n",
    "    \n",
    "    #Create a dictionary mapping each word to a unique id\n",
    "    dictionary = Dictionary([text_splited])\n",
    "    \n",
    "    \n",
    "    corpus = dictionary.doc2bow(text_splited)\n",
    "    \n",
    "    return corpus, dictionary\n",
    "\n",
    "def lda(df_line):\n",
    "    corpus, dictionary = df_line  # Unpacking the tuple directly\n",
    "    lda_model = LdaModel([corpus], num_topics=1, id2word=dictionary)\n",
    "    return lda_model\n",
    "\n",
    "def get_topics(lda_model):\n",
    "    \n",
    "    return lda_model.show_topics(num_topics=1, num_words=50, formatted=True)\n",
    "\n",
    "def extract(topics_tuple):\n",
    "    topics = []\n",
    "    pattern = re.compile(r'(\\d+\\.\\d+)\\*\"(.*?)\"')\n",
    "    for topic_id, topics_str in topics_tuple:\n",
    "        matches = pattern.findall(topics_str)\n",
    "        for match in matches:\n",
    "            freq = float(match[0])\n",
    "            word = match[1]\n",
    "            topics.append((word, freq))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words for the decade 1900s:  [(0, '0.006*\"World\" + 0.005*\"Machine\" + 0.005*\"Wayback\" + 0.005*\"Population\" + 0.004*\"History\" + 0.003*\"ISBN\" + 0.003*\"Data\" + 0.003*\"flight\" + 0.003*\"estimate\" + 0.003*\"Sheet\" + 0.003*\"machine\" + 0.003*\"Haub\" + 0.002*\"Empire\" + 0.002*\"States\" + 0.002*\"engine\" + 0.002*\"Fessenden\" + 0.002*\"m\" + 0.002*\"pp\" + 0.002*\"US\" + 0.002*\"Edison\" + 0.002*\"Company\" + 0.002*\"air\" + 0.002*\"de\" + 0.002*\"p\" + 0.002*\"decade\" + 0.002*\"Years\" + 0.002*\"Popular\" + 0.002*\"device\" + 0.002*\"html\" + 0.002*\"http\" + 0.002*\"Time\" + 0.002*\"Panama\" + 0.001*\"Great\" + 0.001*\"Worlds\" + 0.001*\"developed\" + 0.001*\"earthquake\" + 0.001*\"population\" + 0.001*\"car\" + 0.001*\"time\" + 0.001*\"production\" + 0.001*\"httpsAvebarchive\" + 0.001*\"paper\" + 0.001*\"seconds\" + 0.001*\"radio\" + 0.001*\"estimates\" + 0.001*\"Gabel\" + 0.001*\"use\" + 0.001*\"people\" + 0.001*\"flights\" + 0.001*\"built\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1910s:  [(0, '0.009*\"World\" + 0.006*\"Revolution\" + 0.004*\"people\" + 0.004*\"States\" + 0.004*\"baseball\" + 0.004*\"player\" + 0.003*\"Russia\" + 0.003*\"decade\" + 0.003*\"events\" + 0.003*\"movements\" + 0.003*\"revolution\" + 0.003*\"Henry\" + 0.003*\"Army\" + 0.003*\"Empire\" + 0.003*\"years\" + 0.003*\"Ottoman\" + 0.003*\"establishment\" + 0.002*\"JSTOR\" + 0.002*\"states\" + 0.002*\"film\" + 0.002*\"minutes\" + 0.002*\"Williams\" + 0.002*\"Titanic\" + 0.002*\"monarchy\" + 0.002*\"Eddie\" + 0.002*\"decades\" + 0.002*\"Art\" + 0.002*\"US\" + 0.002*\"Era\" + 0.002*\"Constitution\" + 0.002*\"p\" + 0.002*\"RMS\" + 0.002*\"Austria\" + 0.002*\"Ferdinand\" + 0.002*\"Franz\" + 0.002*\"death\" + 0.002*\"pp\" + 0.002*\"Archduke\" + 0.002*\"online\" + 0.002*\"Samuel\" + 0.002*\"ISBN\" + 0.002*\"Olympics\" + 0.002*\"PDF\" + 0.002*\"empires\" + 0.002*\"art\" + 0.002*\"world\" + 0.002*\"Duchamp\" + 0.002*\"sinking\" + 0.002*\"history\" + 0.002*\"countries\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1920s:  [(0, '0.008*\"p\" + 0.005*\"film\" + 0.005*\"rentals\" + 0.005*\"ISBN\" + 0.005*\"States\" + 0.004*\"Civil\" + 0.004*\"decade\" + 0.003*\"deaths\" + 0.003*\"worldwide\" + 0.003*\"profit\" + 0.003*\"cost\" + 0.003*\"women\" + 0.003*\"History\" + 0.003*\"Press\" + 0.003*\"history\" + 0.003*\"Twenties\" + 0.003*\"earthquake\" + 0.003*\"World\" + 0.003*\"movie\" + 0.003*\"forces\" + 0.003*\"radio\" + 0.003*\"Cost\" + 0.003*\"US\" + 0.002*\"silent\" + 0.002*\"Independence\" + 0.002*\"suffrage\" + 0.002*\"Great\" + 0.002*\"war\" + 0.002*\"world\" + 0.002*\"people\" + 0.002*\"Fascist\" + 0.002*\"Hall\" + 0.002*\"Block\" + 0.002*\"Jazz\" + 0.002*\"Ireland\" + 0.002*\"Company\" + 0.002*\"ch\" + 0.002*\"State\" + 0.002*\"films\" + 0.002*\"Prohibition\" + 0.002*\"Fool\" + 0.002*\"years\" + 0.002*\"magnitude\" + 0.002*\"government\" + 0.002*\"include\" + 0.002*\"womens\" + 0.002*\"Army\" + 0.002*\"tornado\" + 0.002*\"Gross\" + 0.002*\"Generation\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1930s:  [(0, '0.007*\"p\" + 0.005*\"Great\" + 0.005*\"World\" + 0.005*\"Depression\" + 0.005*\"film\" + 0.004*\"boxoffice\" + 0.004*\"States\" + 0.004*\"Cost\" + 0.004*\"ISBN\" + 0.003*\"II\" + 0.003*\"decade\" + 0.003*\"rentals\" + 0.003*\"Nazi\" + 0.003*\"Hitler\" + 0.003*\"Block\" + 0.003*\"cost\" + 0.003*\"Press\" + 0.003*\"worldwide\" + 0.003*\"world\" + 0.002*\"countries\" + 0.002*\"Empire\" + 0.002*\"people\" + 0.002*\"History\" + 0.002*\"Snow\" + 0.002*\"Dust\" + 0.002*\"Bowl\" + 0.002*\"US\" + 0.002*\"Wilson\" + 0.002*\"release\" + 0.002*\"Civil\" + 0.002*\"State\" + 0.002*\"years\" + 0.002*\"profit\" + 0.002*\"fought\" + 0.002*\"territory\" + 0.002*\"Austria\" + 0.002*\"Thirties\" + 0.002*\"Hughes\" + 0.002*\"deaths\" + 0.002*\"forces\" + 0.002*\"production\" + 0.002*\"state\" + 0.002*\"events\" + 0.002*\"Gone\" + 0.002*\"Profit\" + 0.002*\"Film\" + 0.002*\"war\" + 0.002*\"history\" + 0.001*\"Dwarfs\" + 0.001*\"Jews\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1950s:  [(0, '0.010*\"1950s\" + 0.007*\"States\" + 0.005*\"war\" + 0.005*\"Revolution\" + 0.004*\"History\" + 0.004*\"decade\" + 0.004*\"people\" + 0.003*\"independence\" + 0.003*\"World\" + 0.003*\"death\" + 0.003*\"coup\" + 0.003*\"US\" + 0.003*\"Organization\" + 0.003*\"leader\" + 0.003*\"countries\" + 0.003*\"transistor\" + 0.003*\"government\" + 0.003*\"North\" + 0.002*\"communist\" + 0.002*\"Suez\" + 0.002*\"détat\" + 0.002*\"power\" + 0.002*\"satellite\" + 0.002*\"p\" + 0.002*\"ISBN\" + 0.002*\"Cuban\" + 0.002*\"J\" + 0.002*\"II\" + 0.002*\"states\" + 0.002*\"1940s\" + 0.002*\"vaccine\" + 0.002*\"inflation\" + 0.002*\"Sputnik\" + 0.002*\"polio\" + 0.002*\"S\" + 0.002*\"Greenland\" + 0.002*\"shot\" + 0.002*\"Life\" + 0.002*\"nations\" + 0.002*\"Nasser\" + 0.002*\"crashed\" + 0.002*\"world\" + 0.002*\"Truman\" + 0.002*\"Crisis\" + 0.001*\"months\" + 0.001*\"Technology\" + 0.001*\"Stalin\" + 0.001*\"board\" + 0.001*\"assassination\" + 0.001*\"period\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1960s:  [(0, '0.009*\"1960s\" + 0.008*\"movement\" + 0.007*\"States\" + 0.006*\"US\" + 0.004*\"Sixties\" + 0.004*\"decade\" + 0.004*\"rights\" + 0.003*\"end\" + 0.003*\"History\" + 0.003*\"people\" + 0.003*\"government\" + 0.003*\"Revolution\" + 0.003*\"Kennedy\" + 0.003*\"time\" + 0.003*\"president\" + 0.002*\"protests\" + 0.002*\"coup\" + 0.002*\"ISBN\" + 0.002*\"death\" + 0.002*\"war\" + 0.002*\"Civil\" + 0.002*\"Mao\" + 0.002*\"Press\" + 0.002*\"world\" + 0.002*\"country\" + 0.002*\"mass\" + 0.002*\"countries\" + 0.002*\"movements\" + 0.002*\"East\" + 0.002*\"revolution\" + 0.002*\"leader\" + 0.002*\"music\" + 0.002*\"F\" + 0.001*\"earthquake\" + 0.001*\"groups\" + 0.001*\"independence\" + 0.001*\"growth\" + 0.001*\"Cuban\" + 0.001*\"Rights\" + 0.001*\"power\" + 0.001*\"discrimination\" + 0.001*\"Apollo\" + 0.001*\"World\" + 0.001*\"pp\" + 0.001*\"Americans\" + 0.001*\"left\" + 0.001*\"society\" + 0.001*\"mission\" + 0.001*\"B\" + 0.001*\"period\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1970s:  [(0, '0.011*\"1970s\" + 0.006*\"decade\" + 0.005*\"States\" + 0.004*\"US\" + 0.004*\"people\" + 0.004*\"world\" + 0.003*\"power\" + 0.003*\"movement\" + 0.003*\"government\" + 0.003*\"oil\" + 0.003*\"crisis\" + 0.003*\"ISBN\" + 0.003*\"East\" + 0.003*\"women\" + 0.003*\"countries\" + 0.003*\"end\" + 0.002*\"war\" + 0.002*\"years\" + 0.002*\"video\" + 0.002*\"worlds\" + 0.002*\"game\" + 0.002*\"p\" + 0.002*\"woman\" + 0.002*\"Revolution\" + 0.002*\"coup\" + 0.002*\"1960s\" + 0.002*\"crashed\" + 0.002*\"Pakistan\" + 0.002*\"saw\" + 0.002*\"pp\" + 0.002*\"country\" + 0.002*\"Civil\" + 0.002*\"II\" + 0.002*\"cars\" + 0.002*\"year\" + 0.002*\"excerpt\" + 0.002*\"Flight\" + 0.002*\"Seventies\" + 0.002*\"home\" + 0.002*\"History\" + 0.001*\"time\" + 0.001*\"cyclone\" + 0.001*\"include\" + 0.001*\"Liberation\" + 0.001*\"Peoples\" + 0.001*\"state\" + 0.001*\"history\" + 0.001*\"Quebec\" + 0.001*\"market\" + 0.001*\"space\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1980s:  [(0, '0.006*\"decade\" + 0.005*\"people\" + 0.005*\"government\" + 0.005*\"US\" + 0.004*\"end\" + 0.003*\"States\" + 0.003*\"market\" + 0.003*\"Flight\" + 0.003*\"world\" + 0.003*\"video\" + 0.003*\"computer\" + 0.002*\"game\" + 0.002*\"World\" + 0.002*\"time\" + 0.002*\"years\" + 0.002*\"Civil\" + 0.002*\"ISBN\" + 0.002*\"countries\" + 0.002*\"Times\" + 0.002*\"System\" + 0.002*\"p\" + 0.002*\"systems\" + 0.002*\"North\" + 0.002*\"president\" + 0.002*\"communist\" + 0.002*\"computers\" + 0.002*\"Nintendo\" + 0.002*\"games\" + 0.002*\"become\" + 0.002*\"board\" + 0.002*\"country\" + 0.002*\"II\" + 0.002*\"Cold\" + 0.002*\"system\" + 0.002*\"leader\" + 0.002*\"Iran\" + 0.001*\"industry\" + 0.001*\"members\" + 0.001*\"Space\" + 0.001*\"war\" + 0.001*\"reforms\" + 0.001*\"protests\" + 0.001*\"attack\" + 0.001*\"forces\" + 0.001*\"shot\" + 0.001*\"disaster\" + 0.001*\"music\" + 0.001*\"include\" + 0.001*\"use\" + 0.001*\"cars\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1990s:  [(0, '0.005*\"States\" + 0.004*\"decade\" + 0.004*\"people\" + 0.003*\"game\" + 0.003*\"World\" + 0.003*\"end\" + 0.003*\"countries\" + 0.003*\"series\" + 0.003*\"video\" + 0.003*\"years\" + 0.002*\"US\" + 0.002*\"Sciences\" + 0.002*\"Motion\" + 0.002*\"Picture\" + 0.002*\"Academy\" + 0.002*\"Arts\" + 0.002*\"forces\" + 0.002*\"Times\" + 0.002*\"Super\" + 0.002*\"Clinton\" + 0.002*\"bombing\" + 0.002*\"games\" + 0.002*\"world\" + 0.002*\"North\" + 0.002*\"time\" + 0.002*\"Mario\" + 0.002*\"market\" + 0.002*\"genocide\" + 0.002*\"Ireland\" + 0.002*\"music\" + 0.002*\"independence\" + 0.002*\"SSR\" + 0.002*\"popularity\" + 0.002*\"government\" + 0.002*\"decades\" + 0.002*\"becomes\" + 0.002*\"Pokémon\" + 0.002*\"Civil\" + 0.002*\"conflict\" + 0.002*\"ISBN\" + 0.002*\"bestselling\" + 0.002*\"media\" + 0.001*\"Yeltsin\" + 0.001*\"country\" + 0.001*\"crashed\" + 0.001*\"Agreement\" + 0.001*\"women\" + 0.001*\"history\" + 0.001*\"computer\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2000s:  [(0, '0.005*\"decade\" + 0.004*\"Machine\" + 0.004*\"Wayback\" + 0.004*\"people\" + 0.003*\"US\" + 0.003*\"World\" + 0.003*\"States\" + 0.003*\"News\" + 0.003*\"world\" + 0.002*\"countries\" + 0.002*\"PDF\" + 0.002*\"BBC\" + 0.002*\"conflict\" + 0.002*\"http\" + 0.002*\"Times\" + 0.002*\"games\" + 0.001*\"years\" + 0.001*\"Press\" + 0.001*\"Population\" + 0.001*\"game\" + 0.001*\"video\" + 0.001*\"war\" + 0.001*\"attacks\" + 0.001*\"popularity\" + 0.001*\"end\" + 0.001*\"Data\" + 0.001*\"government\" + 0.001*\"crisis\" + 0.001*\"PlayStation\" + 0.001*\"time\" + 0.001*\"de\" + 0.001*\"become\" + 0.001*\"UK\" + 0.001*\"Internet\" + 0.001*\"use\" + 0.001*\"population\" + 0.001*\"Civil\" + 0.001*\"https\" + 0.001*\"groups\" + 0.001*\"rise\" + 0.001*\"Iraq\" + 0.001*\"estimate\" + 0.001*\"earthquake\" + 0.001*\"htt\" + 0.001*\"media\" + 0.001*\"httpswebarchiveorg\" + 0.001*\"bestselling\" + 0.001*\"Philippines\" + 0.001*\"httpsw\" + 0.001*\"public\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2010s:  [(0, '0.006*\"News\" + 0.005*\"States\" + 0.004*\"BBC\" + 0.004*\"people\" + 0.004*\"election\" + 0.003*\"US\" + 0.003*\"decade\" + 0.003*\"Times\" + 0.003*\"protests\" + 0.003*\"earthquake\" + 0.002*\"president\" + 0.002*\"attack\" + 0.002*\"government\" + 0.002*\"Flight\" + 0.002*\"games\" + 0.002*\"fire\" + 0.002*\"Philippines\" + 0.002*\"Russia\" + 0.002*\"world\" + 0.002*\"history\" + 0.002*\"Arab\" + 0.002*\"North\" + 0.002*\"PDF\" + 0.002*\"time\" + 0.002*\"war\" + 0.002*\"Reuters\" + 0.001*\"Trump\" + 0.001*\"ISSN\" + 0.001*\"Pakistan\" + 0.001*\"crisis\" + 0.001*\"Iraq\" + 0.001*\"http\" + 0.001*\"Death\" + 0.001*\"Wii\" + 0.001*\"CNN\" + 0.001*\"Nintendo\" + 0.001*\"damage\" + 0.001*\"countries\" + 0.001*\"Event\" + 0.001*\"de\" + 0.001*\"saw\" + 0.001*\"Date\" + 0.001*\"toll\" + 0.001*\"Country\" + 0.001*\"country\" + 0.001*\"Al\" + 0.001*\"Typhoon\" + 0.001*\"Description\" + 0.001*\"coup\" + 0.001*\"Sudan\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#First for loop to get the most used words for each decade\n",
    "\n",
    "paths= ['data/wikipedia_timeline/1900s.txt', 'data/wikipedia_timeline/1910s.txt', 'data/wikipedia_timeline/1920s.txt', 'data/wikipedia_timeline/1930s.txt', 'data/wikipedia_timeline/1950s.txt', 'data/wikipedia_timeline/1960s.txt', 'data/wikipedia_timeline/1970s.txt', 'data/wikipedia_timeline/1980s.txt', 'data/wikipedia_timeline/1990s.txt', 'data/wikipedia_timeline/2000s.txt', 'data/wikipedia_timeline/2010s.txt']\n",
    "\n",
    "\n",
    "for path in paths:\n",
    "    decade_name = path.split('/')[-1].split('.')[0]\n",
    "    decade=pd.read_csv(path, delimiter='\\t', header=None)\n",
    "    decade=decade.to_string(index=False, header=False)\n",
    "    splitted_decade=text_split_new(decade)\n",
    "    bag_decade=bag_of_words(splitted_decade)\n",
    "    lda_decade=lda(bag_decade)\n",
    "    print(f\"Most used words for the decade {decade_name}: \",get_topics(lda_decade))\n",
    "    print(\"the type of frequencies \", type(lda_decade.get_topics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       World  Machine  Wayback  Population  History   ISBN   Data  estimate  \\\n",
      "1900s  0.006    0.005    0.005       0.005    0.004  0.003  0.003     0.003   \n",
      "1910s  0.009    0.000    0.000       0.000    0.000  0.002  0.000     0.000   \n",
      "1920s  0.003    0.000    0.000       0.000    0.003  0.005  0.000     0.000   \n",
      "1930s  0.005    0.000    0.000       0.000    0.002  0.004  0.000     0.000   \n",
      "1950s  0.003    0.000    0.000       0.000    0.004  0.002  0.000     0.000   \n",
      "\n",
      "       flight  Sheet  ...  CNN  damage  Event  Date  Country  toll   Al  \\\n",
      "1900s   0.003  0.003  ...  0.0     0.0    0.0   0.0      0.0   0.0  0.0   \n",
      "1910s   0.000  0.000  ...  0.0     0.0    0.0   0.0      0.0   0.0  0.0   \n",
      "1920s   0.000  0.000  ...  0.0     0.0    0.0   0.0      0.0   0.0  0.0   \n",
      "1930s   0.000  0.000  ...  0.0     0.0    0.0   0.0      0.0   0.0  0.0   \n",
      "1950s   0.000  0.000  ...  0.0     0.0    0.0   0.0      0.0   0.0  0.0   \n",
      "\n",
      "       Typhoon  Description  Syria  \n",
      "1900s      0.0          0.0    0.0  \n",
      "1910s      0.0          0.0    0.0  \n",
      "1920s      0.0          0.0    0.0  \n",
      "1930s      0.0          0.0    0.0  \n",
      "1950s      0.0          0.0    0.0  \n",
      "\n",
      "[5 rows x 291 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store word frequencies for each decade\n",
    "word_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for path in paths:\n",
    "    # Extract the decade from the file path\n",
    "    decade_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # Read the file\n",
    "    decade = pd.read_csv(path, delimiter='\\t', header=None)\n",
    "    decade = decade.to_string(index=False, header=False)\n",
    "    \n",
    "    # Process the text\n",
    "    splitted_decade = text_split_new(decade)\n",
    "    bag_decade = bag_of_words(splitted_decade)\n",
    "    lda_decade = lda(bag_decade)\n",
    "    \n",
    "    # Get the topics and their frequencies\n",
    "    topics_str = get_topics(lda_decade)\n",
    "    # extract() function was written to extract the topics and their frequencies given their type\n",
    "    topics = extract(topics_str)\n",
    "    \n",
    "    #print(f\"Most used words for the decade {decade_name}: \", topics)\n",
    "    # Update the word frequencies for each decade\n",
    "    for word, freq in topics:\n",
    "        word_frequencies[decade_name][word] = freq\n",
    "        #print(word_frequencies)\n",
    "\n",
    "# Convert the word frequencies dictionary to a DataFrame\n",
    "df_word_frequencies = pd.DataFrame(word_frequencies).fillna(0)\n",
    "\n",
    "# Transpose the DataFrame to have decades as rows and words as columns\n",
    "df_word_frequencies = df_word_frequencies.T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_word_frequencies.head())\n",
    "\n",
    "#save in csv file\n",
    "df_word_frequencies.to_csv(\"word_frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from the txt file the decades\n",
    "\n",
    "summaries_per_decade = pd.read_csv('src/data/summaries_per_decade.txt', delimiter='\\t', header=None)\n",
    "\n",
    "summaries_1900s=summaries_per_decade[0][0]\n",
    "summaries_1910s=summaries_per_decade[0][1]\n",
    "summaries_1920s=summaries_per_decade[0][2]\n",
    "summaries_1930s=summaries_per_decade[0][3]\n",
    "summaries_1940s=summaries_per_decade[0][4]\n",
    "summaries_1950s=summaries_per_decade[0][5]\n",
    "summaries_1960s=summaries_per_decade[0][6]\n",
    "summaries_1970s=summaries_per_decade[0][7]\n",
    "summaries_1980s=summaries_per_decade[0][8]\n",
    "summaries_1990s=summaries_per_decade[0][9]\n",
    "summaries_2000s=summaries_per_decade[0][10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words for the decade 1900:  [(0, '0.017*\"film\" + 0.008*\"Pedro\" + 0.007*\"girl\" + 0.006*\"travelers\" + 0.006*\"astronomers\" + 0.006*\"Mr\" + 0.005*\"shot\" + 0.005*\"Jekyll\" + 0.005*\"train\" + 0.005*\"submarine\" + 0.005*\"Hyde\" + 0.005*\"man\" + 0.005*\"police\" + 0.004*\"Ned\" + 0.004*\"journey\" + 0.004*\"time\" + 0.004*\"Selenite\" + 0.004*\"front\" + 0.004*\"version\" + 0.004*\"scene\" + 0.004*\"woman\" + 0.004*\"space\" + 0.004*\"moves\" + 0.004*\"town\" + 0.004*\"help\" + 0.004*\"capsule\" + 0.004*\"sequence\" + 0.004*\"border\" + 0.004*\"ice\" + 0.003*\"gambler\" + 0.003*\"women\" + 0.003*\"cannon\" + 0.003*\"board\" + 0.003*\"customer\" + 0.003*\"suggest\" + 0.003*\"saloon\" + 0.003*\"mountains\" + 0.003*\"friends\" + 0.003*\"depicts\" + 0.003*\"shows\" + 0.003*\"sheriff\" + 0.003*\"vehicles\" + 0.003*\"falls\" + 0.003*\"revenge\" + 0.003*\"attacks\" + 0.003*\"proposes\" + 0.003*\"astronomer\" + 0.003*\"Selenites\" + 0.003*\"home\" + 0.003*\"rescuer\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1910:  [(0, '0.005*\"man\" + 0.005*\"love\" + 0.004*\"father\" + 0.004*\"film\" + 0.004*\"finds\" + 0.004*\"girl\" + 0.004*\"daughter\" + 0.004*\"wife\" + 0.003*\"woman\" + 0.003*\"home\" + 0.003*\"house\" + 0.003*\"tries\" + 0.003*\"family\" + 0.003*\"money\" + 0.003*\"becomes\" + 0.003*\"room\" + 0.003*\"returns\" + 0.002*\"day\" + 0.002*\"falls\" + 0.002*\"life\" + 0.002*\"Ossi\" + 0.002*\"mother\" + 0.002*\"leaves\" + 0.002*\"help\" + 0.002*\"marry\" + 0.002*\"wealthy\" + 0.002*\"time\" + 0.002*\"way\" + 0.002*\"herself\" + 0.002*\"town\" + 0.002*\"Chaplin\" + 0.002*\"fight\" + 0.002*\"child\" + 0.002*\"son\" + 0.002*\"years\" + 0.002*\"children\" + 0.002*\"husband\" + 0.002*\"story\" + 0.002*\"night\" + 0.002*\"Josef\" + 0.002*\"turns\" + 0.002*\"Jane\" + 0.002*\"doesnt\" + 0.002*\"meets\" + 0.002*\"men\" + 0.002*\"tells\" + 0.002*\"police\" + 0.001*\"runs\" + 0.001*\"boy\" + 0.001*\"Quaker\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1920:  [(0, '0.005*\"love\" + 0.005*\"man\" + 0.004*\"father\" + 0.004*\"film\" + 0.004*\"tells\" + 0.003*\"finds\" + 0.003*\"wife\" + 0.003*\"woman\" + 0.003*\"returns\" + 0.003*\"time\" + 0.003*\"house\" + 0.003*\"marry\" + 0.003*\"home\" + 0.003*\"life\" + 0.002*\"leaves\" + 0.002*\"girl\" + 0.002*\"becomes\" + 0.002*\"daughter\" + 0.002*\"night\" + 0.002*\"tries\" + 0.002*\"son\" + 0.002*\"money\" + 0.002*\"day\" + 0.002*\"decides\" + 0.002*\"room\" + 0.002*\"men\" + 0.002*\"Stan\" + 0.002*\"friend\" + 0.002*\"mother\" + 0.002*\"husband\" + 0.002*\"falls\" + 0.002*\"way\" + 0.002*\"asks\" + 0.002*\"town\" + 0.002*\"sees\" + 0.002*\"end\" + 0.002*\"become\" + 0.002*\"turns\" + 0.002*\"death\" + 0.002*\"family\" + 0.001*\"marriage\" + 0.001*\"return\" + 0.001*\"show\" + 0.001*\"refuses\" + 0.001*\"years\" + 0.001*\"ship\" + 0.001*\"meets\" + 0.001*\"begins\" + 0.001*\"Marie\" + 0.001*\"Mrs\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1930:  [(0, '0.004*\"love\" + 0.004*\"tells\" + 0.003*\"man\" + 0.003*\"father\" + 0.003*\"finds\" + 0.003*\"home\" + 0.003*\"tries\" + 0.003*\"money\" + 0.002*\"wife\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"film\" + 0.002*\"Betty\" + 0.002*\"night\" + 0.002*\"house\" + 0.002*\"police\" + 0.002*\"marry\" + 0.002*\"day\" + 0.002*\"becomes\" + 0.002*\"returns\" + 0.002*\"men\" + 0.002*\"friend\" + 0.002*\"leaves\" + 0.002*\"help\" + 0.002*\"daughter\" + 0.002*\"decides\" + 0.002*\"show\" + 0.002*\"way\" + 0.002*\"falls\" + 0.002*\"woman\" + 0.002*\"room\" + 0.002*\"family\" + 0.002*\"asks\" + 0.002*\"mother\" + 0.002*\"return\" + 0.002*\"Mrs\" + 0.002*\"gang\" + 0.002*\"Dr\" + 0.002*\"son\" + 0.002*\"meets\" + 0.002*\"husband\" + 0.002*\"work\" + 0.002*\"Mr\" + 0.001*\"arrives\" + 0.001*\"town\" + 0.001*\"party\" + 0.001*\"years\" + 0.001*\"turns\" + 0.001*\"sees\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1940:  [(0, '0.004*\"tells\" + 0.003*\"man\" + 0.003*\"love\" + 0.003*\"finds\" + 0.003*\"tries\" + 0.003*\"home\" + 0.003*\"Bugs\" + 0.003*\"time\" + 0.002*\"police\" + 0.002*\"Jerry\" + 0.002*\"wife\" + 0.002*\"night\" + 0.002*\"house\" + 0.002*\"film\" + 0.002*\"way\" + 0.002*\"life\" + 0.002*\"father\" + 0.002*\"money\" + 0.002*\"becomes\" + 0.002*\"leaves\" + 0.002*\"friend\" + 0.002*\"turns\" + 0.002*\"returns\" + 0.002*\"Dr\" + 0.002*\"woman\" + 0.002*\"help\" + 0.002*\"family\" + 0.002*\"men\" + 0.002*\"death\" + 0.002*\"decides\" + 0.002*\"story\" + 0.002*\"room\" + 0.002*\"mother\" + 0.002*\"begins\" + 0.002*\"town\" + 0.002*\"day\" + 0.002*\"work\" + 0.002*\"Daffy\" + 0.001*\"husband\" + 0.001*\"head\" + 0.001*\"escape\" + 0.001*\"sees\" + 0.001*\"falls\" + 0.001*\"asks\" + 0.001*\"train\" + 0.001*\"son\" + 0.001*\"job\" + 0.001*\"end\" + 0.001*\"runs\" + 0.001*\"arrives\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1950:  [(0, '0.003*\"Bugs\" + 0.003*\"tells\" + 0.003*\"man\" + 0.003*\"time\" + 0.003*\"love\" + 0.003*\"tries\" + 0.003*\"Jerry\" + 0.003*\"men\" + 0.003*\"home\" + 0.002*\"finds\" + 0.002*\"wife\" + 0.002*\"life\" + 0.002*\"police\" + 0.002*\"house\" + 0.002*\"film\" + 0.002*\"father\" + 0.002*\"night\" + 0.002*\"money\" + 0.002*\"decides\" + 0.002*\"town\" + 0.002*\"way\" + 0.002*\"day\" + 0.002*\"begins\" + 0.002*\"returns\" + 0.002*\"help\" + 0.002*\"becomes\" + 0.002*\"falls\" + 0.002*\"woman\" + 0.002*\"Joe\" + 0.002*\"leaves\" + 0.002*\"asks\" + 0.002*\"family\" + 0.002*\"end\" + 0.002*\"daughter\" + 0.002*\"return\" + 0.002*\"sees\" + 0.002*\"death\" + 0.001*\"son\" + 0.001*\"head\" + 0.001*\"husband\" + 0.001*\"runs\" + 0.001*\"Sylvester\" + 0.001*\"ship\" + 0.001*\"friend\" + 0.001*\"train\" + 0.001*\"story\" + 0.001*\"turns\" + 0.001*\"work\" + 0.001*\"arrives\" + 0.001*\"Dr\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1960:  [(0, '0.003*\"man\" + 0.003*\"home\" + 0.003*\"film\" + 0.003*\"wife\" + 0.003*\"tells\" + 0.003*\"time\" + 0.002*\"men\" + 0.002*\"father\" + 0.002*\"love\" + 0.002*\"finds\" + 0.002*\"way\" + 0.002*\"life\" + 0.002*\"help\" + 0.002*\"house\" + 0.002*\"money\" + 0.002*\"night\" + 0.002*\"police\" + 0.002*\"tries\" + 0.002*\"becomes\" + 0.002*\"family\" + 0.002*\"mother\" + 0.002*\"escape\" + 0.002*\"daughter\" + 0.002*\"return\" + 0.002*\"begins\" + 0.002*\"woman\" + 0.002*\"death\" + 0.002*\"decides\" + 0.002*\"day\" + 0.002*\"son\" + 0.002*\"Dr\" + 0.002*\"town\" + 0.002*\"car\" + 0.002*\"leaves\" + 0.002*\"returns\" + 0.001*\"friend\" + 0.001*\"end\" + 0.001*\"story\" + 0.001*\"falls\" + 0.001*\"kill\" + 0.001*\"meets\" + 0.001*\"group\" + 0.001*\"children\" + 0.001*\"arrives\" + 0.001*\"plan\" + 0.001*\"fight\" + 0.001*\"asks\" + 0.001*\"become\" + 0.001*\"discovers\" + 0.001*\"turns\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1970:  [(0, '0.003*\"film\" + 0.003*\"man\" + 0.003*\"home\" + 0.003*\"life\" + 0.003*\"tells\" + 0.003*\"finds\" + 0.003*\"men\" + 0.003*\"time\" + 0.002*\"police\" + 0.002*\"family\" + 0.002*\"house\" + 0.002*\"wife\" + 0.002*\"father\" + 0.002*\"love\" + 0.002*\"way\" + 0.002*\"woman\" + 0.002*\"becomes\" + 0.002*\"help\" + 0.002*\"death\" + 0.002*\"car\" + 0.002*\"tries\" + 0.002*\"begins\" + 0.002*\"day\" + 0.002*\"night\" + 0.002*\"town\" + 0.002*\"kill\" + 0.002*\"friend\" + 0.002*\"money\" + 0.002*\"escape\" + 0.002*\"mother\" + 0.002*\"son\" + 0.002*\"returns\" + 0.002*\"meets\" + 0.002*\"friends\" + 0.002*\"room\" + 0.001*\"Dr\" + 0.001*\"leaves\" + 0.001*\"decides\" + 0.001*\"group\" + 0.001*\"daughter\" + 0.001*\"return\" + 0.001*\"story\" + 0.001*\"arrives\" + 0.001*\"become\" + 0.001*\"train\" + 0.001*\"work\" + 0.001*\"end\" + 0.001*\"years\" + 0.001*\"kills\" + 0.001*\"meet\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1980:  [(0, '0.003*\"time\" + 0.003*\"home\" + 0.003*\"house\" + 0.003*\"film\" + 0.003*\"tells\" + 0.003*\"father\" + 0.003*\"finds\" + 0.003*\"man\" + 0.003*\"life\" + 0.003*\"police\" + 0.002*\"car\" + 0.002*\"night\" + 0.002*\"family\" + 0.002*\"love\" + 0.002*\"help\" + 0.002*\"tries\" + 0.002*\"mother\" + 0.002*\"friends\" + 0.002*\"day\" + 0.002*\"wife\" + 0.002*\"son\" + 0.002*\"begins\" + 0.002*\"death\" + 0.002*\"becomes\" + 0.002*\"way\" + 0.002*\"school\" + 0.002*\"friend\" + 0.002*\"kill\" + 0.002*\"decides\" + 0.002*\"leaves\" + 0.002*\"group\" + 0.002*\"returns\" + 0.002*\"money\" + 0.002*\"escape\" + 0.002*\"men\" + 0.002*\"years\" + 0.002*\"woman\" + 0.001*\"town\" + 0.001*\"room\" + 0.001*\"return\" + 0.001*\"daughter\" + 0.001*\"meets\" + 0.001*\"body\" + 0.001*\"attempts\" + 0.001*\"end\" + 0.001*\"story\" + 0.001*\"asks\" + 0.001*\"sees\" + 0.001*\"ends\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1990:  [(0, '0.004*\"tells\" + 0.003*\"father\" + 0.003*\"home\" + 0.003*\"time\" + 0.003*\"family\" + 0.003*\"life\" + 0.003*\"film\" + 0.003*\"finds\" + 0.003*\"house\" + 0.003*\"love\" + 0.003*\"man\" + 0.003*\"mother\" + 0.002*\"police\" + 0.002*\"day\" + 0.002*\"help\" + 0.002*\"tries\" + 0.002*\"night\" + 0.002*\"money\" + 0.002*\"wife\" + 0.002*\"friends\" + 0.002*\"becomes\" + 0.002*\"son\" + 0.002*\"begins\" + 0.002*\"way\" + 0.002*\"death\" + 0.002*\"friend\" + 0.002*\"decides\" + 0.002*\"kill\" + 0.002*\"car\" + 0.002*\"leaves\" + 0.002*\"years\" + 0.002*\"daughter\" + 0.002*\"men\" + 0.002*\"school\" + 0.002*\"asks\" + 0.002*\"story\" + 0.002*\"escape\" + 0.002*\"return\" + 0.002*\"meets\" + 0.002*\"woman\" + 0.001*\"children\" + 0.001*\"returns\" + 0.001*\"Dr\" + 0.001*\"arrives\" + 0.001*\"group\" + 0.001*\"work\" + 0.001*\"discovers\" + 0.001*\"fight\" + 0.001*\"attempts\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 200:  [(0, '0.004*\"tells\" + 0.004*\"father\" + 0.004*\"time\" + 0.004*\"film\" + 0.003*\"life\" + 0.003*\"finds\" + 0.003*\"home\" + 0.003*\"family\" + 0.003*\"love\" + 0.003*\"house\" + 0.003*\"mother\" + 0.003*\"man\" + 0.003*\"day\" + 0.002*\"help\" + 0.002*\"friends\" + 0.002*\"police\" + 0.002*\"tries\" + 0.002*\"begins\" + 0.002*\"night\" + 0.002*\"friend\" + 0.002*\"car\" + 0.002*\"school\" + 0.002*\"son\" + 0.002*\"story\" + 0.002*\"way\" + 0.002*\"asks\" + 0.002*\"leaves\" + 0.002*\"wife\" + 0.002*\"kill\" + 0.002*\"becomes\" + 0.002*\"years\" + 0.002*\"death\" + 0.002*\"decides\" + 0.002*\"money\" + 0.002*\"group\" + 0.002*\"daughter\" + 0.002*\"meets\" + 0.002*\"girl\" + 0.002*\"escape\" + 0.002*\"woman\" + 0.002*\"room\" + 0.002*\"sees\" + 0.002*\"end\" + 0.001*\"reveals\" + 0.001*\"men\" + 0.001*\"team\" + 0.001*\"returns\" + 0.001*\"people\" + 0.001*\"ends\" + 0.001*\"return\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "decades = [summaries_1900s,summaries_1910s,summaries_1920s,summaries_1930s,summaries_1940s,summaries_1950s,summaries_1960s,summaries_1970s,summaries_1980s,summaries_1990s,summaries_2000s]\n",
    "\n",
    "decade_name = [1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000]\n",
    "i=0\n",
    "\n",
    "for decade in decades:\n",
    "    splitted_decade=text_split_new(decade)\n",
    "    bag_decade=bag_of_words(splitted_decade)\n",
    "    lda_decade=lda(bag_decade)\n",
    "    print(f\"Most used words for the decade {decade_name[i]}: \",get_topics(lda_decade))\n",
    "    print(\"the type of frequencies \", type(lda_decade.get_topics()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       film  Pedro   girl  astronomers  travelers     Mr   shot    man  train  \\\n",
      "1900  0.017  0.008  0.007        0.006      0.006  0.006  0.005  0.005  0.005   \n",
      "1910  0.004  0.000  0.004        0.000      0.000  0.000  0.000  0.005  0.000   \n",
      "1920  0.004  0.000  0.002        0.000      0.000  0.000  0.000  0.005  0.000   \n",
      "1930  0.002  0.000  0.000        0.000      0.000  0.002  0.000  0.003  0.000   \n",
      "1940  0.002  0.000  0.000        0.000      0.000  0.000  0.000  0.003  0.001   \n",
      "\n",
      "      Jekyll  ...  discovers  friends  meet  kills  school  body  attempts  \\\n",
      "1900   0.005  ...        0.0      0.0   0.0    0.0     0.0   0.0       0.0   \n",
      "1910   0.000  ...        0.0      0.0   0.0    0.0     0.0   0.0       0.0   \n",
      "1920   0.000  ...        0.0      0.0   0.0    0.0     0.0   0.0       0.0   \n",
      "1930   0.000  ...        0.0      0.0   0.0    0.0     0.0   0.0       0.0   \n",
      "1940   0.000  ...        0.0      0.0   0.0    0.0     0.0   0.0       0.0   \n",
      "\n",
      "      reveals  team  people  \n",
      "1900      0.0   0.0     0.0  \n",
      "1910      0.0   0.0     0.0  \n",
      "1920      0.0   0.0     0.0  \n",
      "1930      0.0   0.0     0.0  \n",
      "1940      0.0   0.0     0.0  \n",
      "\n",
      "[5 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store word frequencies for each decade\n",
    "word_frequencies_summaries = defaultdict(lambda: defaultdict(int))\n",
    "i=0\n",
    "\n",
    "for decade in decades:\n",
    "    # Process the text\n",
    "    splitted_decade = text_split_new(decade)\n",
    "    bag_decade = bag_of_words(splitted_decade)\n",
    "    lda_decade = lda(bag_decade)\n",
    "    \n",
    "    # Get the topics and their frequencies\n",
    "    topics_str = get_topics(lda_decade)\n",
    "    # extract() function was written to extract the topics and their frequencies given their type\n",
    "    topics = extract(topics_str)\n",
    "    \n",
    "    #print(f\"Most used words for the decade {decade_name}: \", topics)\n",
    "    # Update the word frequencies for each decade\n",
    "    for word, freq in topics:\n",
    "        word_frequencies_summaries[decade_name[i]][word] = freq\n",
    "    i+=1\n",
    "\n",
    "# Convert the word frequencies dictionary to a DataFrame\n",
    "df_word_frequencies = pd.DataFrame(word_frequencies_summaries).fillna(0)\n",
    "\n",
    "# Transpose the DataFrame to have decades as rows and words as columns\n",
    "df_word_frequencies = df_word_frequencies.T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_word_frequencies.head())\n",
    "\n",
    "#save in csv file\n",
    "df_word_frequencies.to_csv(\"word_frequencies_summaries.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to load the notebook utils.ipynb\n",
    "import nbformat\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Load the notebook utils.ipynb\n",
    "with open('../scripts/utils.ipynb') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Create an instance of InteractiveShell\n",
    "shell = InteractiveShell.instance()\n",
    "\n",
    "# Execute the notebook utils.ipynb\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == 'code':\n",
    "        shell.run_cell(cell.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary files\n",
    "merged_df = pd.read_csv('merged_df.tsv', sep='\\t')\n",
    "plot_summary_df = pd.read_csv('plot_summaries.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the summaries in plot_summaries dataset by decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Estimation of the release year of the films without release data\n",
    "mean_release_year_by_genre = pd.read_csv('mean_release_year_by_genre.tsv', sep='\\t')\n",
    "merged_df['Estimated_release_year'] = merged_df.apply(estimate_release_year, axis=1, args=(mean_release_year_by_genre,))\n",
    "merged_df['Estimated_release_year'] = pd.to_datetime(merged_df['Estimated_release_year'], errors='coerce').dt.year\n",
    "merged_df['Decade'] = (merged_df['Estimated_release_year'] // 10) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing decades \n",
    "decades=np.arange(1900,2020,10).astype(float)\n",
    "\n",
    "# extracting a list of the movie ids for each decade\n",
    "movies_ids_per_decade={} # dict of the lists of movie ids for all decades\n",
    "for decade in decades:\n",
    "    movies_df=merged_df[merged_df['Decade']==decade]\n",
    "    decade_ids=movies_df['Wikipedia_movie_ID'].to_list()\n",
    "    decade_ids=list(dict.fromkeys(decade_ids))\n",
    "    movies_ids_per_decade[decade]=decade_ids\n",
    "\n",
    "# creating a list of the summaries of the decade for each decade\n",
    "summaries_per_decade={} # dict of the lists of movie summaries for all decades\n",
    "for decade in decades :\n",
    "    summaries_list=[]\n",
    "    for id in movies_ids_per_decade[decade] :\n",
    "        summary=plot_summary_df[plot_summary_df['movie_id']==id]['plot_summary'].to_list() # extracting the plot summaries from the movie ids in movies_ids_per_decade\n",
    "        if(summary!=[]):\n",
    "            summaries_list.append(summary[0])\n",
    "    summaries_per_decade[decade]=summaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving obtained list\n",
    "with open('summaries_per_decade.txt', 'w') as fichier:\n",
    "    for key, valeur in summaries_per_decade.items():\n",
    "        fichier.write(f'{key}: {valeur}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list with all the wikipedia summaries sorted by decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"wikipedia_summaries\"\n",
    "summary_per_decade = [[]]\n",
    "i = 0 \n",
    "\n",
    "for folder, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder, file)\n",
    "        summary = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
    "        str_summary = summary.to_string(index=False, header=False)\n",
    "        summary_per_decade[i-1].append(str_summary)\n",
    "    summary_per_decade.append([])\n",
    "    i += 1 \n",
    "\n",
    "summary_per_decade = summary_per_decade[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying LDA to the wikipedia timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words for the decade 1900s:  [(0, '0.010*\"W\" + 0.007*\"httpwww\" + 0.004*\"Machine\" + 0.004*\"Wayback\" + 0.004*\"Population\" + 0.004*\"V\" + 0.003*\"History\" + 0.003*\"orld\" + 0.003*\"T\" + 0.003*\"ISBN\" + 0.003*\"Data\" + 0.003*\"flight\" + 0.002*\"estimate\" + 0.002*\"de\" + 0.002*\"States\" + 0.002*\"s\" + 0.002*\"Sheet\" + 0.002*\"Haub\" + 0.002*\"machine\" + 0.002*\"Empire\" + 0.002*\"decade\" + 0.002*\"Fessenden\" + 0.002*\"US\" + 0.002*\"f\" + 0.002*\"engine\" + 0.002*\"p\" + 0.002*\"pp\" + 0.002*\"http\" + 0.002*\"Company\" + 0.002*\"Edison\" + 0.002*\"people\" + 0.002*\"air\" + 0.002*\"m\" + 0.001*\"Popular\" + 0.001*\"Great\" + 0.001*\"Years\" + 0.001*\"World\" + 0.001*\"time\" + 0.001*\"Panama\" + 0.001*\"device\" + 0.001*\"production\" + 0.001*\"population\" + 0.001*\"S\" + 0.001*\"world\" + 0.001*\"paper\" + 0.001*\"car\" + 0.001*\"years\" + 0.001*\"part\" + 0.001*\"earthquake\" + 0.001*\"httpswww\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1910s:  [(0, '0.015*\"W\" + 0.007*\"ar\" + 0.007*\"World\" + 0.005*\"V\" + 0.005*\"Revolution\" + 0.004*\"people\" + 0.004*\"player\" + 0.004*\"States\" + 0.004*\"baseball\" + 0.003*\"Russia\" + 0.003*\"decade\" + 0.003*\"httpswww\" + 0.003*\"Army\" + 0.003*\"T\" + 0.003*\"Empire\" + 0.003*\"revolution\" + 0.003*\"events\" + 0.002*\"establishment\" + 0.002*\"orld\" + 0.002*\"Henry\" + 0.002*\"movements\" + 0.002*\"years\" + 0.002*\"Ottoman\" + 0.002*\"minutes\" + 0.002*\"Era\" + 0.002*\"US\" + 0.002*\"states\" + 0.002*\"JSTOR\" + 0.002*\"monarchy\" + 0.002*\"RMS\" + 0.002*\"p\" + 0.002*\"Constitution\" + 0.002*\"e\" + 0.002*\"death\" + 0.002*\"Art\" + 0.002*\"decades\" + 0.002*\"Eddie\" + 0.002*\"Franz\" + 0.002*\"ISBN\" + 0.002*\"Ferdinand\" + 0.002*\"Samuel\" + 0.002*\"pp\" + 0.002*\"Austria\" + 0.002*\"Williams\" + 0.002*\"online\" + 0.002*\"film\" + 0.002*\"Chaplin\" + 0.002*\"countries\" + 0.002*\"Treaty\" + 0.002*\"wars\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1920s:  [(0, '0.012*\"W\" + 0.007*\"ar\" + 0.005*\"p\" + 0.004*\"film\" + 0.004*\"decade\" + 0.004*\"player\" + 0.003*\"States\" + 0.003*\"ISBN\" + 0.003*\"rentals\" + 0.003*\"baseball\" + 0.003*\"Civil\" + 0.003*\"T\" + 0.002*\"httpswww\" + 0.002*\"worldwide\" + 0.002*\"movie\" + 0.002*\"httpwww\" + 0.002*\"deaths\" + 0.002*\"Press\" + 0.002*\"silent\" + 0.002*\"World\" + 0.002*\"de\" + 0.002*\"Twenties\" + 0.002*\"History\" + 0.002*\"s\" + 0.002*\"Jazz\" + 0.002*\"V\" + 0.002*\"cost\" + 0.002*\"history\" + 0.002*\"profit\" + 0.002*\"earthquake\" + 0.002*\"S\" + 0.002*\"Alexander\" + 0.002*\"world\" + 0.002*\"Cost\" + 0.002*\"films\" + 0.002*\"radio\" + 0.002*\"time\" + 0.002*\"cricketer\" + 0.002*\"US\" + 0.002*\"women\" + 0.002*\"forces\" + 0.002*\"war\" + 0.002*\"years\" + 0.002*\"Fool\" + 0.002*\"Independence\" + 0.002*\"Olympics\" + 0.002*\"Great\" + 0.001*\"fice\" + 0.001*\"Company\" + 0.001*\"begins\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1930s:  [(0, '0.012*\"W\" + 0.005*\"p\" + 0.004*\"s\" + 0.004*\"Great\" + 0.004*\"ar\" + 0.004*\"Depression\" + 0.004*\"film\" + 0.003*\"States\" + 0.003*\"fice\" + 0.003*\"T\" + 0.003*\"World\" + 0.003*\"boxof\" + 0.003*\"Cost\" + 0.003*\"V\" + 0.003*\"decade\" + 0.002*\"ISBN\" + 0.002*\"II\" + 0.002*\"rentals\" + 0.002*\"Nazi\" + 0.002*\"baseball\" + 0.002*\"worldwide\" + 0.002*\"Gone\" + 0.002*\"Empire\" + 0.002*\"httpswww\" + 0.002*\"Hitler\" + 0.002*\"Block\" + 0.002*\"Press\" + 0.002*\"httpwww\" + 0.002*\"Snow\" + 0.002*\"films\" + 0.002*\"cost\" + 0.002*\"years\" + 0.002*\"State\" + 0.002*\"art\" + 0.002*\"world\" + 0.001*\"music\" + 0.001*\"countries\" + 0.001*\"US\" + 0.001*\"people\" + 0.001*\"History\" + 0.001*\"release\" + 0.001*\"Bowl\" + 0.001*\"R\" + 0.001*\"boxing\" + 0.001*\"movement\" + 0.001*\"Hughes\" + 0.001*\"Dust\" + 0.001*\"Wilson\" + 0.001*\"Civil\" + 0.001*\"events\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1940s:  [(0, '0.013*\"W\" + 0.007*\"1940s\" + 0.007*\"II\" + 0.006*\"ar\" + 0.005*\"war\" + 0.005*\"World\" + 0.003*\"Field\" + 0.003*\"Marshal\" + 0.003*\"Holocaust\" + 0.003*\"decade\" + 0.003*\"Nazi\" + 0.003*\"States\" + 0.003*\"de\" + 0.002*\"films\" + 0.002*\"Joe\" + 0.002*\"Robinson\" + 0.002*\"orld\" + 0.002*\"world\" + 0.002*\"development\" + 0.002*\"events\" + 0.002*\"Press\" + 0.002*\"fashion\" + 0.002*\"postwar\" + 0.002*\"Williams\" + 0.002*\"people\" + 0.002*\"Crosby\" + 0.002*\"Powell\" + 0.002*\"independence\" + 0.002*\"Bob\" + 0.002*\"half\" + 0.002*\"Fleet\" + 0.002*\"Harbor\" + 0.002*\"Nations\" + 0.002*\"millions\" + 0.002*\"Arab\" + 0.002*\"V\" + 0.002*\"computer\" + 0.002*\"attack\" + 0.002*\"state\" + 0.002*\"State\" + 0.002*\"others\" + 0.001*\"Nazis\" + 0.001*\"Les\" + 0.001*\"film\" + 0.001*\"Hitler\" + 0.001*\"Benny\" + 0.001*\"Isoroku\" + 0.001*\"forces\" + 0.001*\"color\" + 0.001*\"Ray\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1950s:  [(0, '0.009*\"1950s\" + 0.009*\"W\" + 0.003*\"States\" + 0.003*\"player\" + 0.003*\"V\" + 0.003*\"Johnny\" + 0.003*\"baseball\" + 0.002*\"war\" + 0.002*\"decade\" + 0.002*\"time\" + 0.002*\"History\" + 0.002*\"people\" + 0.002*\"Revolution\" + 0.002*\"ar\" + 0.002*\"httpswww\" + 0.002*\"orld\" + 0.002*\"Ray\" + 0.002*\"world\" + 0.002*\"Jimmy\" + 0.002*\"Brothers\" + 0.002*\"independence\" + 0.002*\"Art\" + 0.002*\"II\" + 0.002*\"Bobby\" + 0.002*\"fashion\" + 0.001*\"Presley\" + 0.001*\"US\" + 0.001*\"death\" + 0.001*\"coup\" + 0.001*\"Williams\" + 0.001*\"music\" + 0.001*\"Organization\" + 0.001*\"Chanel\" + 0.001*\"television\" + 0.001*\"rock\" + 0.001*\"Bill\" + 0.001*\"North\" + 0.001*\"leader\" + 0.001*\"roll\" + 0.001*\"World\" + 0.001*\"Davis\" + 0.001*\"Joe\" + 0.001*\"TV\" + 0.001*\"httpwww\" + 0.001*\"boxer\" + 0.001*\"J\" + 0.001*\"Dean\" + 0.001*\"Jerry\" + 0.001*\"power\" + 0.001*\"T\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1960s:  [(0, '0.007*\"1960s\" + 0.006*\"W\" + 0.004*\"US\" + 0.004*\"movement\" + 0.004*\"States\" + 0.004*\"decade\" + 0.003*\"V\" + 0.003*\"music\" + 0.003*\"rock\" + 0.003*\"s\" + 0.002*\"time\" + 0.002*\"Sixties\" + 0.002*\"end\" + 0.002*\"rights\" + 0.002*\"Beatles\" + 0.002*\"people\" + 0.002*\"History\" + 0.002*\"httpswww\" + 0.002*\"Bob\" + 0.002*\"ar\" + 0.002*\"country\" + 0.002*\"Kennedy\" + 0.002*\"album\" + 0.002*\"government\" + 0.002*\"World\" + 0.001*\"Show\" + 0.001*\"hit\" + 0.001*\"death\" + 0.001*\"groups\" + 0.001*\"Revolution\" + 0.001*\"world\" + 0.001*\"httpwww\" + 0.001*\"film\" + 0.001*\"protests\" + 0.001*\"coup\" + 0.001*\"president\" + 0.001*\"T\" + 0.001*\"films\" + 0.001*\"ISBN\" + 0.001*\"movements\" + 0.001*\"group\" + 0.001*\"counterculture\" + 0.001*\"Dylan\" + 0.001*\"Press\" + 0.001*\"revolution\" + 0.001*\"war\" + 0.001*\"pop\" + 0.001*\"Mao\" + 0.001*\"period\" + 0.001*\"League\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1970s:  [(0, '0.010*\"1970s\" + 0.006*\"W\" + 0.006*\"decade\" + 0.003*\"States\" + 0.003*\"US\" + 0.002*\"world\" + 0.002*\"V\" + 0.002*\"music\" + 0.002*\"people\" + 0.002*\"movement\" + 0.002*\"saw\" + 0.002*\"ar\" + 0.002*\"httpswww\" + 0.002*\"rock\" + 0.002*\"power\" + 0.002*\"television\" + 0.002*\"s\" + 0.002*\"end\" + 0.002*\"1960s\" + 0.002*\"government\" + 0.002*\"women\" + 0.002*\"oil\" + 0.001*\"East\" + 0.001*\"crisis\" + 0.001*\"ISBN\" + 0.001*\"World\" + 0.001*\"countries\" + 0.001*\"time\" + 0.001*\"half\" + 0.001*\"bands\" + 0.001*\"years\" + 0.001*\"worlds\" + 0.001*\"game\" + 0.001*\"genre\" + 0.001*\"war\" + 0.001*\"country\" + 0.001*\"video\" + 0.001*\"Show\" + 0.001*\"film\" + 0.001*\"Bob\" + 0.001*\"woman\" + 0.001*\"II\" + 0.001*\"p\" + 0.001*\"coup\" + 0.001*\"T\" + 0.001*\"artists\" + 0.001*\"Brothers\" + 0.001*\"popularity\" + 0.001*\"cars\" + 0.001*\"Revolution\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1980s:  [(0, '0.007*\"W\" + 0.006*\"decade\" + 0.003*\"US\" + 0.003*\"films\" + 0.003*\"people\" + 0.003*\"music\" + 0.003*\"end\" + 0.002*\"States\" + 0.002*\"government\" + 0.002*\"ar\" + 0.002*\"won\" + 0.002*\"world\" + 0.002*\"video\" + 0.002*\"number\" + 0.002*\"years\" + 0.002*\"time\" + 0.002*\"s\" + 0.002*\"httpswww\" + 0.002*\"market\" + 0.002*\"film\" + 0.002*\"series\" + 0.002*\"Flight\" + 0.002*\"popularity\" + 0.002*\"include\" + 0.002*\"television\" + 0.001*\"computer\" + 0.001*\"countries\" + 0.001*\"T\" + 0.001*\"World\" + 0.001*\"Cup\" + 0.001*\"era\" + 0.001*\"V\" + 0.001*\"saw\" + 0.001*\"e\" + 0.001*\"game\" + 0.001*\"North\" + 0.001*\"industry\" + 0.001*\"history\" + 0.001*\"ISBN\" + 0.001*\"Civil\" + 0.001*\"home\" + 0.001*\"album\" + 0.001*\"rock\" + 0.001*\"games\" + 0.001*\"Duran\" + 0.001*\"year\" + 0.001*\"d\" + 0.001*\"p\" + 0.001*\"Super\" + 0.001*\"II\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1990s:  [(0, '0.007*\"W\" + 0.004*\"decade\" + 0.004*\"httpswww\" + 0.003*\"States\" + 0.003*\"series\" + 0.002*\"game\" + 0.002*\"s\" + 0.002*\"end\" + 0.002*\"people\" + 0.002*\"ar\" + 0.002*\"years\" + 0.002*\"music\" + 0.002*\"World\" + 0.002*\"T\" + 0.002*\"time\" + 0.002*\"video\" + 0.002*\"US\" + 0.002*\"films\" + 0.002*\"countries\" + 0.002*\"world\" + 0.002*\"saw\" + 0.002*\"Super\" + 0.002*\"httpwww\" + 0.002*\"shows\" + 0.002*\"Academy\" + 0.001*\"popularity\" + 0.001*\"V\" + 0.001*\"Picture\" + 0.001*\"Motion\" + 0.001*\"Arts\" + 0.001*\"Sciences\" + 0.001*\"television\" + 0.001*\"history\" + 0.001*\"games\" + 0.001*\"Mario\" + 0.001*\"forces\" + 0.001*\"bombing\" + 0.001*\"genre\" + 0.001*\"North\" + 0.001*\"Clinton\" + 0.001*\"decades\" + 0.001*\"success\" + 0.001*\"imes\" + 0.001*\"show\" + 0.001*\"Pokémon\" + 0.001*\"won\" + 0.001*\"orld\" + 0.001*\"media\" + 0.001*\"bestselling\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2000s:  [(0, '0.006*\"httpswww\" + 0.006*\"decade\" + 0.006*\"W\" + 0.005*\"httpwww\" + 0.003*\"Wayback\" + 0.003*\"Machine\" + 0.003*\"US\" + 0.003*\"people\" + 0.002*\"world\" + 0.002*\"series\" + 0.002*\"s\" + 0.002*\"States\" + 0.002*\"BBC\" + 0.002*\"television\" + 0.002*\"News\" + 0.002*\"countries\" + 0.002*\"T\" + 0.002*\"PDF\" + 0.002*\"years\" + 0.002*\"World\" + 0.001*\"film\" + 0.001*\"conflict\" + 0.001*\"time\" + 0.001*\"films\" + 0.001*\"saw\" + 0.001*\"music\" + 0.001*\"ar\" + 0.001*\"game\" + 0.001*\"popularity\" + 0.001*\"UK\" + 0.001*\"orld\" + 0.001*\"video\" + 0.001*\"show\" + 0.001*\"http\" + 0.001*\"games\" + 0.001*\"V\" + 0.001*\"attacks\" + 0.001*\"war\" + 0.001*\"history\" + 0.001*\"record\" + 0.001*\"end\" + 0.001*\"rise\" + 0.001*\"imes\" + 0.001*\"e\" + 0.001*\"Press\" + 0.001*\"Population\" + 0.001*\"become\" + 0.001*\"use\" + 0.001*\"government\" + 0.001*\"de\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2010s:  [(0, '0.013*\"httpswww\" + 0.005*\"News\" + 0.004*\"decade\" + 0.004*\"httpwww\" + 0.003*\"BBC\" + 0.003*\"W\" + 0.003*\"States\" + 0.003*\"people\" + 0.003*\"T\" + 0.003*\"US\" + 0.003*\"election\" + 0.002*\"imes\" + 0.002*\"time\" + 0.002*\"president\" + 0.002*\"protests\" + 0.002*\"attack\" + 0.002*\"government\" + 0.002*\"earthquake\" + 0.002*\"games\" + 0.001*\"Flight\" + 0.001*\"world\" + 0.001*\"saw\" + 0.001*\"V\" + 0.001*\"html\" + 0.001*\"Trump\" + 0.001*\"World\" + 0.001*\"PDF\" + 0.001*\"Arab\" + 0.001*\"Reuters\" + 0.001*\"North\" + 0.001*\"fire\" + 0.001*\"crisis\" + 0.001*\"ISSN\" + 0.001*\"war\" + 0.001*\"Russia\" + 0.001*\"http\" + 0.001*\"Wii\" + 0.001*\"history\" + 0.001*\"won\" + 0.001*\"Iraq\" + 0.001*\"CNN\" + 0.001*\"game\" + 0.001*\"Nintendo\" + 0.001*\"de\" + 0.001*\"Death\" + 0.001*\"film\" + 0.001*\"countries\" + 0.001*\"Country\" + 0.001*\"Philippines\" + 0.001*\"music\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#First for loop to get the most used words for each decade\n",
    "\n",
    "paths= ['wikipedia_timeline/1900s.txt', 'wikipedia_timeline/1910s.txt', 'wikipedia_timeline/1920s.txt', 'wikipedia_timeline/1930s.txt', 'wikipedia_timeline/1940s.txt', 'wikipedia_timeline/1950s.txt', 'wikipedia_timeline/1960s.txt', 'wikipedia_timeline/1970s.txt', 'wikipedia_timeline/1980s.txt', 'wikipedia_timeline/1990s.txt', 'wikipedia_timeline/2000s.txt', 'wikipedia_timeline/2010s.txt']\n",
    "\n",
    "\n",
    "for path in paths:\n",
    "    # Extract the decade from the file path\n",
    "    decade_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # Read the file\n",
    "    decade=pd.read_csv(path, delimiter='\\t', header=None)\n",
    "    decade=decade.to_string(index=False, header=False)\n",
    "    \n",
    "    # Process the text\n",
    "    splitted_decade=text_split_new(decade)\n",
    "    bag_decade=bag_of_words(splitted_decade)\n",
    "    lda_decade=lda_new(bag_decade)\n",
    "    \n",
    "    # print the topics and their frequencies\n",
    "    print(f\"Most used words for the decade {decade_name}: \",get_topics_new(lda_decade))\n",
    "    print(\"the type of frequencies \", type(lda_decade.get_topics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W  httpwww  Machine  Wayback  Population      V  History   orld  \\\n",
      "1900s  0.010    0.007    0.004    0.004       0.004  0.004    0.003  0.003   \n",
      "1910s  0.015    0.000    0.000    0.000       0.000  0.005    0.000  0.002   \n",
      "1920s  0.012    0.002    0.000    0.000       0.000  0.002    0.002  0.000   \n",
      "1930s  0.012    0.002    0.000    0.000       0.000  0.003    0.001  0.000   \n",
      "1940s  0.013    0.000    0.000    0.000       0.000  0.002    0.000  0.002   \n",
      "\n",
      "           T   ISBN  ...  Reuters  fire  ISSN  Wii  Iraq  CNN  Death  \\\n",
      "1900s  0.003  0.003  ...      0.0   0.0   0.0  0.0   0.0  0.0    0.0   \n",
      "1910s  0.003  0.002  ...      0.0   0.0   0.0  0.0   0.0  0.0    0.0   \n",
      "1920s  0.003  0.003  ...      0.0   0.0   0.0  0.0   0.0  0.0    0.0   \n",
      "1930s  0.003  0.002  ...      0.0   0.0   0.0  0.0   0.0  0.0    0.0   \n",
      "1940s  0.000  0.000  ...      0.0   0.0   0.0  0.0   0.0  0.0    0.0   \n",
      "\n",
      "       Nintendo  Country  Philippines  \n",
      "1900s       0.0      0.0          0.0  \n",
      "1910s       0.0      0.0          0.0  \n",
      "1920s       0.0      0.0          0.0  \n",
      "1930s       0.0      0.0          0.0  \n",
      "1940s       0.0      0.0          0.0  \n",
      "\n",
      "[5 rows x 266 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store word frequencies for each decade\n",
    "word_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for path in paths:\n",
    "    # Extract the decade from the file path\n",
    "    decade_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # Read the file\n",
    "    decade = pd.read_csv(path, delimiter='\\t', header=None)\n",
    "    decade = decade.to_string(index=False, header=False)\n",
    "    \n",
    "    # Process the text\n",
    "    splitted_decade = text_split_new(decade)\n",
    "    bag_decade = bag_of_words(splitted_decade)\n",
    "    lda_decade = lda_new(bag_decade)\n",
    "    \n",
    "    # Get the topics and their frequencies\n",
    "    topics_str = get_topics_new(lda_decade)\n",
    "    # extract() function was written to extract the topics and their frequencies given their type\n",
    "    topics = extract_new(topics_str)\n",
    "    \n",
    "    #print(f\"Most used words for the decade {decade_name}: \", topics)\n",
    "    # Update the word frequencies for each decade\n",
    "    for word, freq in topics:\n",
    "        word_frequencies[decade_name][word] = freq\n",
    "        #print(word_frequencies)\n",
    "\n",
    "# Convert the word frequencies dictionary to a DataFrame\n",
    "df_word_frequencies = pd.DataFrame(word_frequencies).fillna(0)\n",
    "\n",
    "# Transpose the DataFrame to have decades as rows and words as columns\n",
    "df_word_frequencies = df_word_frequencies.T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_word_frequencies.head())\n",
    "\n",
    "#save in csv file\n",
    "df_word_frequencies.to_csv(\"word_frequencies_wikipedia.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying LDA to summaries_per_decade (plot_summaries dataset by decade sorted by decade c.f. above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from the txt file the decades\n",
    "\n",
    "summaries_per_decade = pd.read_csv('summaries_per_decade.txt', delimiter='\\t', header=None)\n",
    "\n",
    "#initialization summaries splitted per decades\n",
    "summaries_1900s=summaries_per_decade[0][0]\n",
    "summaries_1910s=summaries_per_decade[0][1]\n",
    "summaries_1920s=summaries_per_decade[0][2]\n",
    "summaries_1930s=summaries_per_decade[0][3]\n",
    "summaries_1940s=summaries_per_decade[0][4]\n",
    "summaries_1950s=summaries_per_decade[0][5]\n",
    "summaries_1960s=summaries_per_decade[0][6]\n",
    "summaries_1970s=summaries_per_decade[0][7]\n",
    "summaries_1980s=summaries_per_decade[0][8]\n",
    "summaries_1990s=summaries_per_decade[0][9]\n",
    "summaries_2000s=summaries_per_decade[0][10]\n",
    "summaries_2010s=summaries_per_decade[0][11]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words for the decade 1900:  [(0, '0.017*\"film\" + 0.008*\"Pedro\" + 0.007*\"girl\" + 0.006*\"Mr\" + 0.006*\"travelers\" + 0.006*\"astronomers\" + 0.005*\"submarine\" + 0.005*\"shot\" + 0.005*\"Hyde\" + 0.005*\"man\" + 0.005*\"Jekyll\" + 0.005*\"train\" + 0.005*\"police\" + 0.004*\"version\" + 0.004*\"front\" + 0.004*\"scene\" + 0.004*\"journey\" + 0.004*\"border\" + 0.004*\"help\" + 0.004*\"town\" + 0.004*\"capsule\" + 0.004*\"woman\" + 0.004*\"ice\" + 0.004*\"Ned\" + 0.004*\"space\" + 0.004*\"time\" + 0.004*\"sequence\" + 0.004*\"moves\" + 0.004*\"Selenite\" + 0.003*\"bar\" + 0.003*\"attacks\" + 0.003*\"ground\" + 0.003*\"Selenites\" + 0.003*\"rescues\" + 0.003*\"proposes\" + 0.003*\"board\" + 0.003*\"Henri\" + 0.003*\"automobile\" + 0.003*\"rescuer\" + 0.003*\"Story\" + 0.003*\"gang\" + 0.003*\"women\" + 0.003*\"thug\" + 0.003*\"Alice\" + 0.003*\"home\" + 0.003*\"sun\" + 0.003*\"customer\" + 0.003*\"begins\" + 0.003*\"capture\" + 0.003*\"sheriff\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1910:  [(0, '0.005*\"man\" + 0.005*\"love\" + 0.004*\"father\" + 0.004*\"film\" + 0.004*\"finds\" + 0.004*\"girl\" + 0.004*\"daughter\" + 0.004*\"wife\" + 0.003*\"woman\" + 0.003*\"home\" + 0.003*\"house\" + 0.003*\"tries\" + 0.003*\"family\" + 0.003*\"money\" + 0.003*\"room\" + 0.003*\"becomes\" + 0.003*\"returns\" + 0.002*\"day\" + 0.002*\"falls\" + 0.002*\"life\" + 0.002*\"Ossi\" + 0.002*\"mother\" + 0.002*\"leaves\" + 0.002*\"help\" + 0.002*\"marry\" + 0.002*\"wealthy\" + 0.002*\"herself\" + 0.002*\"time\" + 0.002*\"way\" + 0.002*\"Chaplin\" + 0.002*\"town\" + 0.002*\"son\" + 0.002*\"fight\" + 0.002*\"child\" + 0.002*\"children\" + 0.002*\"years\" + 0.002*\"story\" + 0.002*\"husband\" + 0.002*\"Josef\" + 0.002*\"night\" + 0.002*\"turns\" + 0.002*\"doesnt\" + 0.002*\"meets\" + 0.002*\"men\" + 0.002*\"Jane\" + 0.002*\"police\" + 0.002*\"tells\" + 0.001*\"runs\" + 0.001*\"boy\" + 0.001*\"sees\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1920:  [(0, '0.005*\"love\" + 0.005*\"man\" + 0.004*\"father\" + 0.004*\"film\" + 0.004*\"tells\" + 0.003*\"finds\" + 0.003*\"wife\" + 0.003*\"woman\" + 0.003*\"returns\" + 0.003*\"time\" + 0.003*\"house\" + 0.003*\"marry\" + 0.003*\"home\" + 0.003*\"life\" + 0.002*\"leaves\" + 0.002*\"girl\" + 0.002*\"becomes\" + 0.002*\"daughter\" + 0.002*\"night\" + 0.002*\"tries\" + 0.002*\"son\" + 0.002*\"money\" + 0.002*\"day\" + 0.002*\"room\" + 0.002*\"decides\" + 0.002*\"Stan\" + 0.002*\"men\" + 0.002*\"friend\" + 0.002*\"mother\" + 0.002*\"husband\" + 0.002*\"falls\" + 0.002*\"way\" + 0.002*\"asks\" + 0.002*\"town\" + 0.002*\"end\" + 0.002*\"sees\" + 0.002*\"become\" + 0.002*\"family\" + 0.002*\"turns\" + 0.002*\"death\" + 0.001*\"marriage\" + 0.001*\"return\" + 0.001*\"show\" + 0.001*\"refuses\" + 0.001*\"ship\" + 0.001*\"years\" + 0.001*\"begins\" + 0.001*\"Marie\" + 0.001*\"meets\" + 0.001*\"story\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1930:  [(0, '0.004*\"love\" + 0.004*\"tells\" + 0.003*\"man\" + 0.003*\"father\" + 0.003*\"finds\" + 0.003*\"home\" + 0.003*\"tries\" + 0.003*\"money\" + 0.002*\"wife\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"film\" + 0.002*\"Betty\" + 0.002*\"night\" + 0.002*\"house\" + 0.002*\"police\" + 0.002*\"marry\" + 0.002*\"day\" + 0.002*\"becomes\" + 0.002*\"returns\" + 0.002*\"men\" + 0.002*\"friend\" + 0.002*\"help\" + 0.002*\"leaves\" + 0.002*\"daughter\" + 0.002*\"decides\" + 0.002*\"show\" + 0.002*\"way\" + 0.002*\"falls\" + 0.002*\"woman\" + 0.002*\"family\" + 0.002*\"room\" + 0.002*\"asks\" + 0.002*\"mother\" + 0.002*\"return\" + 0.002*\"Mrs\" + 0.002*\"Dr\" + 0.002*\"gang\" + 0.002*\"son\" + 0.002*\"meets\" + 0.002*\"husband\" + 0.002*\"work\" + 0.002*\"Mr\" + 0.001*\"arrives\" + 0.001*\"town\" + 0.001*\"party\" + 0.001*\"years\" + 0.001*\"turns\" + 0.001*\"sees\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1940:  [(0, '0.004*\"tells\" + 0.003*\"man\" + 0.003*\"love\" + 0.003*\"finds\" + 0.003*\"tries\" + 0.003*\"Bugs\" + 0.003*\"home\" + 0.003*\"time\" + 0.002*\"police\" + 0.002*\"Jerry\" + 0.002*\"wife\" + 0.002*\"night\" + 0.002*\"house\" + 0.002*\"film\" + 0.002*\"way\" + 0.002*\"life\" + 0.002*\"father\" + 0.002*\"money\" + 0.002*\"becomes\" + 0.002*\"leaves\" + 0.002*\"friend\" + 0.002*\"turns\" + 0.002*\"woman\" + 0.002*\"Dr\" + 0.002*\"returns\" + 0.002*\"help\" + 0.002*\"family\" + 0.002*\"men\" + 0.002*\"death\" + 0.002*\"decides\" + 0.002*\"story\" + 0.002*\"room\" + 0.002*\"mother\" + 0.002*\"begins\" + 0.002*\"day\" + 0.002*\"town\" + 0.002*\"work\" + 0.002*\"Daffy\" + 0.001*\"husband\" + 0.001*\"head\" + 0.001*\"escape\" + 0.001*\"sees\" + 0.001*\"falls\" + 0.001*\"asks\" + 0.001*\"train\" + 0.001*\"son\" + 0.001*\"job\" + 0.001*\"end\" + 0.001*\"arrives\" + 0.001*\"runs\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1950:  [(0, '0.003*\"Bugs\" + 0.003*\"tells\" + 0.003*\"man\" + 0.003*\"time\" + 0.003*\"love\" + 0.003*\"tries\" + 0.003*\"Jerry\" + 0.003*\"men\" + 0.003*\"home\" + 0.002*\"finds\" + 0.002*\"wife\" + 0.002*\"life\" + 0.002*\"police\" + 0.002*\"house\" + 0.002*\"film\" + 0.002*\"father\" + 0.002*\"night\" + 0.002*\"money\" + 0.002*\"town\" + 0.002*\"decides\" + 0.002*\"way\" + 0.002*\"day\" + 0.002*\"begins\" + 0.002*\"returns\" + 0.002*\"help\" + 0.002*\"becomes\" + 0.002*\"falls\" + 0.002*\"Joe\" + 0.002*\"woman\" + 0.002*\"leaves\" + 0.002*\"asks\" + 0.002*\"family\" + 0.002*\"end\" + 0.002*\"daughter\" + 0.002*\"return\" + 0.002*\"sees\" + 0.002*\"death\" + 0.001*\"son\" + 0.001*\"head\" + 0.001*\"husband\" + 0.001*\"runs\" + 0.001*\"Sylvester\" + 0.001*\"ship\" + 0.001*\"friend\" + 0.001*\"train\" + 0.001*\"story\" + 0.001*\"turns\" + 0.001*\"work\" + 0.001*\"arrives\" + 0.001*\"Dr\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1960:  [(0, '0.003*\"man\" + 0.003*\"home\" + 0.003*\"film\" + 0.003*\"wife\" + 0.003*\"tells\" + 0.003*\"time\" + 0.002*\"men\" + 0.002*\"father\" + 0.002*\"love\" + 0.002*\"finds\" + 0.002*\"way\" + 0.002*\"life\" + 0.002*\"help\" + 0.002*\"house\" + 0.002*\"money\" + 0.002*\"night\" + 0.002*\"police\" + 0.002*\"tries\" + 0.002*\"becomes\" + 0.002*\"family\" + 0.002*\"escape\" + 0.002*\"mother\" + 0.002*\"daughter\" + 0.002*\"return\" + 0.002*\"begins\" + 0.002*\"woman\" + 0.002*\"death\" + 0.002*\"Dr\" + 0.002*\"decides\" + 0.002*\"day\" + 0.002*\"son\" + 0.002*\"town\" + 0.002*\"car\" + 0.002*\"leaves\" + 0.002*\"returns\" + 0.001*\"friend\" + 0.001*\"end\" + 0.001*\"story\" + 0.001*\"falls\" + 0.001*\"kill\" + 0.001*\"group\" + 0.001*\"meets\" + 0.001*\"children\" + 0.001*\"arrives\" + 0.001*\"plan\" + 0.001*\"fight\" + 0.001*\"asks\" + 0.001*\"become\" + 0.001*\"discovers\" + 0.001*\"turns\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1970:  [(0, '0.003*\"film\" + 0.003*\"man\" + 0.003*\"home\" + 0.003*\"life\" + 0.003*\"tells\" + 0.003*\"finds\" + 0.003*\"men\" + 0.003*\"time\" + 0.002*\"police\" + 0.002*\"family\" + 0.002*\"house\" + 0.002*\"wife\" + 0.002*\"father\" + 0.002*\"love\" + 0.002*\"way\" + 0.002*\"woman\" + 0.002*\"becomes\" + 0.002*\"help\" + 0.002*\"death\" + 0.002*\"car\" + 0.002*\"tries\" + 0.002*\"begins\" + 0.002*\"day\" + 0.002*\"night\" + 0.002*\"town\" + 0.002*\"kill\" + 0.002*\"money\" + 0.002*\"friend\" + 0.002*\"escape\" + 0.002*\"mother\" + 0.002*\"son\" + 0.002*\"returns\" + 0.002*\"friends\" + 0.002*\"meets\" + 0.002*\"room\" + 0.001*\"Dr\" + 0.001*\"decides\" + 0.001*\"leaves\" + 0.001*\"group\" + 0.001*\"daughter\" + 0.001*\"return\" + 0.001*\"arrives\" + 0.001*\"story\" + 0.001*\"become\" + 0.001*\"train\" + 0.001*\"work\" + 0.001*\"end\" + 0.001*\"years\" + 0.001*\"meet\" + 0.001*\"kills\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1980:  [(0, '0.003*\"time\" + 0.003*\"home\" + 0.003*\"house\" + 0.003*\"film\" + 0.003*\"father\" + 0.003*\"tells\" + 0.003*\"finds\" + 0.003*\"man\" + 0.003*\"life\" + 0.003*\"police\" + 0.002*\"car\" + 0.002*\"night\" + 0.002*\"family\" + 0.002*\"love\" + 0.002*\"help\" + 0.002*\"tries\" + 0.002*\"mother\" + 0.002*\"friends\" + 0.002*\"day\" + 0.002*\"wife\" + 0.002*\"son\" + 0.002*\"begins\" + 0.002*\"death\" + 0.002*\"becomes\" + 0.002*\"way\" + 0.002*\"school\" + 0.002*\"friend\" + 0.002*\"kill\" + 0.002*\"decides\" + 0.002*\"leaves\" + 0.002*\"group\" + 0.002*\"returns\" + 0.002*\"money\" + 0.002*\"men\" + 0.002*\"escape\" + 0.002*\"years\" + 0.002*\"woman\" + 0.001*\"town\" + 0.001*\"room\" + 0.001*\"return\" + 0.001*\"daughter\" + 0.001*\"meets\" + 0.001*\"body\" + 0.001*\"attempts\" + 0.001*\"story\" + 0.001*\"end\" + 0.001*\"asks\" + 0.001*\"sees\" + 0.001*\"ends\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1990:  [(0, '0.004*\"tells\" + 0.003*\"father\" + 0.003*\"home\" + 0.003*\"time\" + 0.003*\"family\" + 0.003*\"life\" + 0.003*\"film\" + 0.003*\"house\" + 0.003*\"finds\" + 0.003*\"love\" + 0.003*\"man\" + 0.003*\"mother\" + 0.002*\"police\" + 0.002*\"day\" + 0.002*\"help\" + 0.002*\"tries\" + 0.002*\"night\" + 0.002*\"money\" + 0.002*\"friends\" + 0.002*\"wife\" + 0.002*\"becomes\" + 0.002*\"son\" + 0.002*\"begins\" + 0.002*\"death\" + 0.002*\"way\" + 0.002*\"friend\" + 0.002*\"decides\" + 0.002*\"kill\" + 0.002*\"car\" + 0.002*\"leaves\" + 0.002*\"years\" + 0.002*\"daughter\" + 0.002*\"men\" + 0.002*\"school\" + 0.002*\"asks\" + 0.002*\"story\" + 0.002*\"escape\" + 0.002*\"return\" + 0.002*\"meets\" + 0.002*\"woman\" + 0.001*\"children\" + 0.001*\"returns\" + 0.001*\"Dr\" + 0.001*\"group\" + 0.001*\"arrives\" + 0.001*\"work\" + 0.001*\"fight\" + 0.001*\"discovers\" + 0.001*\"attempts\" + 0.001*\"become\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2000:  [(0, '0.004*\"tells\" + 0.004*\"father\" + 0.004*\"time\" + 0.003*\"film\" + 0.003*\"life\" + 0.003*\"finds\" + 0.003*\"home\" + 0.003*\"family\" + 0.003*\"love\" + 0.003*\"house\" + 0.003*\"mother\" + 0.003*\"man\" + 0.003*\"day\" + 0.002*\"help\" + 0.002*\"friends\" + 0.002*\"police\" + 0.002*\"tries\" + 0.002*\"begins\" + 0.002*\"night\" + 0.002*\"friend\" + 0.002*\"school\" + 0.002*\"car\" + 0.002*\"son\" + 0.002*\"story\" + 0.002*\"way\" + 0.002*\"leaves\" + 0.002*\"asks\" + 0.002*\"becomes\" + 0.002*\"wife\" + 0.002*\"kill\" + 0.002*\"years\" + 0.002*\"death\" + 0.002*\"decides\" + 0.002*\"money\" + 0.002*\"group\" + 0.002*\"daughter\" + 0.002*\"meets\" + 0.002*\"girl\" + 0.002*\"escape\" + 0.002*\"room\" + 0.002*\"woman\" + 0.002*\"sees\" + 0.002*\"end\" + 0.001*\"reveals\" + 0.001*\"men\" + 0.001*\"returns\" + 0.001*\"team\" + 0.001*\"people\" + 0.001*\"ends\" + 0.001*\"return\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2010:  [(0, '0.005*\"tells\" + 0.004*\"father\" + 0.003*\"life\" + 0.003*\"house\" + 0.003*\"film\" + 0.003*\"family\" + 0.003*\"love\" + 0.003*\"finds\" + 0.003*\"home\" + 0.003*\"time\" + 0.003*\"day\" + 0.003*\"mother\" + 0.003*\"help\" + 0.003*\"man\" + 0.002*\"police\" + 0.002*\"night\" + 0.002*\"tries\" + 0.002*\"friends\" + 0.002*\"son\" + 0.002*\"friend\" + 0.002*\"story\" + 0.002*\"leaves\" + 0.002*\"money\" + 0.002*\"asks\" + 0.002*\"wife\" + 0.002*\"begins\" + 0.002*\"decides\" + 0.002*\"way\" + 0.002*\"school\" + 0.002*\"kill\" + 0.002*\"car\" + 0.002*\"years\" + 0.002*\"death\" + 0.002*\"daughter\" + 0.002*\"group\" + 0.002*\"becomes\" + 0.002*\"meets\" + 0.002*\"room\" + 0.002*\"fight\" + 0.002*\"people\" + 0.002*\"reveals\" + 0.002*\"escape\" + 0.002*\"arrives\" + 0.002*\"sees\" + 0.002*\"returns\" + 0.001*\"woman\" + 0.001*\"party\" + 0.001*\"men\" + 0.001*\"become\" + 0.001*\"girl\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#intializing a 'decade' list\n",
    "decades = [summaries_1900s,summaries_1910s,summaries_1920s,summaries_1930s,summaries_1940s,summaries_1950s,summaries_1960s,summaries_1970s,summaries_1980s,summaries_1990s,summaries_2000s,summaries_2010s]\n",
    "\n",
    "#initializing decades name\n",
    "decade_name = [1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010]\n",
    "i=0\n",
    "\n",
    "for decade in decades:\n",
    "    # Process the text\n",
    "    splitted_decade=text_split_new(decade)\n",
    "    bag_decade=bag_of_words(splitted_decade)\n",
    "    lda_decade=lda_new(bag_decade)\n",
    "    \n",
    "    # print the topics and their frequencies\n",
    "    print(f\"Most used words for the decade {decade_name[i]}: \",get_topics_new(lda_decade))\n",
    "    print(\"the type of frequencies \", type(lda_decade.get_topics()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       film  Pedro   girl  travelers     Mr  astronomers   shot    man  train  \\\n",
      "1900  0.017  0.008  0.007      0.006  0.006        0.006  0.005  0.005  0.005   \n",
      "1910  0.004  0.000  0.004      0.000  0.000        0.000  0.000  0.005  0.000   \n",
      "1920  0.004  0.000  0.002      0.000  0.000        0.000  0.000  0.005  0.000   \n",
      "1930  0.002  0.000  0.000      0.000  0.002        0.000  0.000  0.003  0.000   \n",
      "1940  0.002  0.000  0.000      0.000  0.000        0.000  0.000  0.003  0.001   \n",
      "\n",
      "       Hyde  ...  friends  kills  meet  school  body  attempts  ends  reveals  \\\n",
      "1900  0.005  ...      0.0    0.0   0.0     0.0   0.0       0.0   0.0      0.0   \n",
      "1910  0.000  ...      0.0    0.0   0.0     0.0   0.0       0.0   0.0      0.0   \n",
      "1920  0.000  ...      0.0    0.0   0.0     0.0   0.0       0.0   0.0      0.0   \n",
      "1930  0.000  ...      0.0    0.0   0.0     0.0   0.0       0.0   0.0      0.0   \n",
      "1940  0.000  ...      0.0    0.0   0.0     0.0   0.0       0.0   0.0      0.0   \n",
      "\n",
      "      team  people  \n",
      "1900   0.0     0.0  \n",
      "1910   0.0     0.0  \n",
      "1920   0.0     0.0  \n",
      "1930   0.0     0.0  \n",
      "1940   0.0     0.0  \n",
      "\n",
      "[5 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store word frequencies for each decade\n",
    "word_frequencies_summaries = defaultdict(lambda: defaultdict(int))\n",
    "i=0\n",
    "\n",
    "for decade in decades:\n",
    "    # Process the text\n",
    "    splitted_decade = text_split_new(decade)\n",
    "    bag_decade = bag_of_words(splitted_decade)\n",
    "    lda_decade = lda_new(bag_decade)\n",
    "    \n",
    "    # Get the topics and their frequencies\n",
    "    topics_str = get_topics_new(lda_decade)\n",
    "    # extract() function was written to extract the topics and their frequencies given their type\n",
    "    topics = extract_new(topics_str)\n",
    "    \n",
    "    #print(f\"Most used words for the decade {decade_name}: \", topics)\n",
    "    # Update the word frequencies for each decade\n",
    "    for word, freq in topics:\n",
    "        word_frequencies_summaries[decade_name[i]][word] = freq\n",
    "    i+=1\n",
    "\n",
    "# Convert the word frequencies dictionary to a DataFrame\n",
    "df_word_frequencies = pd.DataFrame(word_frequencies_summaries).fillna(0)\n",
    "\n",
    "# Transpose the DataFrame to have decades as rows and words as columns\n",
    "df_word_frequencies = df_word_frequencies.T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_word_frequencies.head())\n",
    "\n",
    "#save in csv file\n",
    "df_word_frequencies.to_csv(\"word_frequencies_summaries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying LDA to the wikipedia summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words for the decade 1900:  [(0, '0.020*\"film\" + 0.007*\"films\" + 0.006*\"Film\" + 0.006*\"p\" + 0.006*\"W\" + 0.005*\"de\" + 0.005*\"ISBN\" + 0.005*\"Silent\" + 0.005*\"Razin\" + 0.004*\"httpswww\" + 0.004*\"Geor\" + 0.004*\"Florence\" + 0.004*\"Biograph\" + 0.004*\"time\" + 0.004*\"story\" + 0.004*\"Guise\" + 0.004*\"Stenka\" + 0.004*\"Press\" + 0.004*\"Méliès\" + 0.003*\"pp\" + 0.003*\"Le\" + 0.003*\"score\" + 0.003*\"Griffith\" + 0.003*\"scene\" + 0.003*\"ge\" + 0.003*\"Georges\" + 0.003*\"D\" + 0.003*\"States\" + 0.003*\"silent\" + 0.003*\"Drankov\" + 0.003*\"du\" + 0.003*\"Company\" + 0.003*\"Gladys\" + 0.003*\"Release\" + 0.002*\"IMDb\" + 0.002*\"Jewess\" + 0.002*\"Fairies\" + 0.002*\"play\" + 0.002*\"Country\" + 0.002*\"Mélièss\" + 0.002*\"s\" + 0.002*\"Bargy\" + 0.002*\"httpwww\" + 0.002*\"Romance\" + 0.002*\"V\" + 0.002*\"date\" + 0.002*\"Assassination\" + 0.002*\"le\" + 0.002*\"production\" + 0.002*\"links\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1910:  [(0, '0.016*\"film\" + 0.009*\"Film\" + 0.007*\"Edna\" + 0.007*\"Campbell\" + 0.006*\"httpswww\" + 0.006*\"Silent\" + 0.005*\"Leagues\" + 0.005*\"Hardy\" + 0.005*\"Chaplin\" + 0.005*\"films\" + 0.004*\"comedy\" + 0.004*\"States\" + 0.004*\"Country\" + 0.004*\"Release\" + 0.004*\"officer\" + 0.004*\"Edwards\" + 0.004*\"p\" + 0.004*\"date\" + 0.004*\"Oliver\" + 0.004*\"intertitles\" + 0.003*\"police\" + 0.003*\"Company\" + 0.003*\"List\" + 0.003*\"links\" + 0.003*\"IMDb\" + 0.003*\"silent\" + 0.003*\"policeman\" + 0.003*\"Lukes\" + 0.003*\"underwater\" + 0.003*\"Languages\" + 0.002*\"Nemo\" + 0.002*\"Refer\" + 0.002*\"mother\" + 0.002*\"Undertow\" + 0.002*\"ences\" + 0.002*\"Lloyd\" + 0.002*\"Adventurer\" + 0.002*\"time\" + 0.002*\"Harold\" + 0.002*\"Johnsons\" + 0.002*\"Denver\" + 0.002*\"Fattys\" + 0.002*\"towards\" + 0.002*\"thinks\" + 0.002*\"Child\" + 0.002*\"Picture\" + 0.002*\"girl\" + 0.002*\"World\" + 0.002*\"Lubin\" + 0.002*\"Fatty\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1920:  [(0, '0.016*\"film\" + 0.009*\"httpswww\" + 0.006*\"Film\" + 0.005*\"Hardy\" + 0.005*\"Silent\" + 0.005*\"Youre\" + 0.005*\"Laurel\" + 0.004*\"AllMovie\" + 0.004*\"Darn\" + 0.004*\"sound\" + 0.004*\"httpwww\" + 0.004*\"W\" + 0.004*\"T\" + 0.003*\"Million\" + 0.003*\"time\" + 0.003*\"Stan\" + 0.003*\"IMDb\" + 0.003*\"Bid\" + 0.003*\"links\" + 0.003*\"Release\" + 0.003*\"date\" + 0.003*\"Men\" + 0.003*\"silent\" + 0.003*\"title\" + 0.003*\"Marry\" + 0.003*\"O\" + 0.003*\"Powell\" + 0.003*\"Refer\" + 0.003*\"Hal\" + 0.003*\"Lips\" + 0.003*\"ences\" + 0.003*\"films\" + 0.002*\"Name\" + 0.002*\"Country\" + 0.002*\"ISBN\" + 0.002*\"States\" + 0.002*\"Is\" + 0.002*\"Nr\" + 0.002*\"Tootin\" + 0.002*\"Production\" + 0.002*\"Thy\" + 0.002*\"Roach\" + 0.002*\"Piel\" + 0.002*\"Ladys\" + 0.002*\"Woman\" + 0.002*\"ootin\" + 0.002*\"Erich\" + 0.002*\"deletion\" + 0.002*\"minutes\" + 0.002*\"intertitles\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1930:  [(0, '0.007*\"film\" + 0.007*\"httpswww\" + 0.005*\"W\" + 0.004*\"Pennies\" + 0.004*\"Heaven\" + 0.004*\"Pictures\" + 0.004*\"AllMovie\" + 0.004*\"Stooges\" + 0.003*\"Ros\" + 0.003*\"time\" + 0.003*\"Beggars\" + 0.003*\"IMDb\" + 0.003*\"Patsy\" + 0.003*\"Crosby\" + 0.003*\"Ermine\" + 0.003*\"Rynox\" + 0.003*\"Times\" + 0.003*\"title\" + 0.003*\"date\" + 0.003*\"Moe\" + 0.002*\"Release\" + 0.002*\"Language\" + 0.002*\"Country\" + 0.002*\"links\" + 0.002*\"Uncivil\" + 0.002*\"minutes\" + 0.002*\"Night\" + 0.002*\"Production\" + 0.002*\"help\" + 0.002*\"ences\" + 0.002*\"V\" + 0.002*\"httpwww\" + 0.002*\"Links\" + 0.002*\"deletion\" + 0.002*\"Madame\" + 0.002*\"love\" + 0.002*\"Tonight\" + 0.002*\"Hayworth\" + 0.002*\"Racketeer\" + 0.002*\"Movie\" + 0.002*\"orphanage\" + 0.002*\"template\" + 0.002*\"States\" + 0.002*\"Woman\" + 0.002*\"Films\" + 0.002*\"B\" + 0.002*\"baby\" + 0.002*\"Raft\" + 0.002*\"n\" + 0.002*\"Captain\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1940:  [(0, '0.014*\"film\" + 0.010*\"p\" + 0.006*\"httpswww\" + 0.005*\"films\" + 0.004*\"Film\" + 0.003*\"Gangs\" + 0.003*\"ISBN\" + 0.003*\"Pictures\" + 0.003*\"Frankenstein\" + 0.003*\"Murdock\" + 0.002*\"Mitchum\" + 0.002*\"W\" + 0.002*\"time\" + 0.002*\"Universal\" + 0.002*\"e\" + 0.002*\"s\" + 0.002*\"AllMovie\" + 0.002*\"httpwww\" + 0.002*\"Edelmann\" + 0.002*\"Mank\" + 0.002*\"Scott\" + 0.002*\"role\" + 0.002*\"Steal\" + 0.002*\"Talbot\" + 0.002*\"Sex\" + 0.002*\"Ann\" + 0.002*\"J\" + 0.002*\"Alice\" + 0.002*\"Films\" + 0.002*\"Times\" + 0.002*\"Carradine\" + 0.002*\"review\" + 0.002*\"T\" + 0.002*\"monster\" + 0.002*\"Faye\" + 0.002*\"Man\" + 0.002*\"production\" + 0.002*\"Carmen\" + 0.002*\"Ros\" + 0.002*\"Variety\" + 0.002*\"Wolf\" + 0.002*\"war\" + 0.001*\"noir\" + 0.001*\"r\" + 0.001*\"Brunas\" + 0.001*\"States\" + 0.001*\"Miranda\" + 0.001*\"ed\" + 0.001*\"Benny\" + 0.001*\"Undercurrent\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1950:  [(0, '0.013*\"film\" + 0.006*\"Film\" + 0.006*\"httpswww\" + 0.005*\"p\" + 0.003*\"W\" + 0.003*\"ISBN\" + 0.003*\"Dadier\" + 0.003*\"Jungle\" + 0.003*\"Yesterday\" + 0.003*\"V\" + 0.003*\"Hobson\" + 0.002*\"Apache\" + 0.002*\"T\" + 0.002*\"time\" + 0.002*\"Cochise\" + 0.002*\"Robinson\" + 0.002*\"httpwww\" + 0.002*\"Jeffords\" + 0.002*\"S\" + 0.002*\"Cukor\" + 0.002*\"play\" + 0.002*\"Holliday\" + 0.002*\"films\" + 0.002*\"s\" + 0.002*\"Awards\" + 0.002*\"Alice\" + 0.002*\"Dr\" + 0.002*\"Choice\" + 0.002*\"States\" + 0.002*\"Academy\" + 0.002*\"Spaceways\" + 0.002*\"httpnlagov\" + 0.002*\"e\" + 0.002*\"Judy\" + 0.001*\"Films\" + 0.001*\"Variety\" + 0.001*\"Hobsons\" + 0.001*\"Chips\" + 0.001*\"song\" + 0.001*\"Movies\" + 0.001*\"Arrow\" + 0.001*\"Rafferty\" + 0.001*\"story\" + 0.001*\"Production\" + 0.001*\"Motion\" + 0.001*\"M\" + 0.001*\"Crenshaw\" + 0.001*\"Bill\" + 0.001*\"cast\" + 0.001*\"Native\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1960:  [(0, '0.013*\"film\" + 0.007*\"p\" + 0.006*\"httpswww\" + 0.004*\"ISBN\" + 0.004*\"Charity\" + 0.003*\"films\" + 0.003*\"W\" + 0.003*\"Film\" + 0.003*\"Silence\" + 0.003*\"Shirō\" + 0.002*\"s\" + 0.002*\"Jigoku\" + 0.002*\"Variety\" + 0.002*\"Shadows\" + 0.002*\"time\" + 0.002*\"Man\" + 0.002*\"Ester\" + 0.002*\"Sweet\" + 0.002*\"Army\" + 0.002*\"Criterion\" + 0.002*\"T\" + 0.002*\"Collection\" + 0.002*\"Resistance\" + 0.002*\"Paint\" + 0.002*\"Ingmar\" + 0.002*\"Films\" + 0.002*\"Johan\" + 0.002*\"DVD\" + 0.002*\"Le\" + 0.002*\"play\" + 0.001*\"e\" + 0.001*\"Press\" + 0.001*\"AllMovie\" + 0.001*\"httpwww\" + 0.001*\"V\" + 0.001*\"Tamura\" + 0.001*\"Elizabeth\" + 0.001*\"Box\" + 0.001*\"Lerner\" + 0.001*\"Language\" + 0.001*\"release\" + 0.001*\"Cinema\" + 0.001*\"Pardner\" + 0.001*\"title\" + 0.001*\"movie\" + 0.001*\"Fosse\" + 0.001*\"Fonda\" + 0.001*\"MacLaine\" + 0.001*\"Yukiko\" + 0.001*\"Jane\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1970:  [(0, '0.013*\"film\" + 0.006*\"Christmas\" + 0.006*\"httpswww\" + 0.006*\"Black\" + 0.004*\"Film\" + 0.004*\"W\" + 0.003*\"p\" + 0.003*\"Concorde\" + 0.003*\"films\" + 0.003*\"time\" + 0.003*\"Airport\" + 0.003*\"Clark\" + 0.002*\"Fate\" + 0.002*\"T\" + 0.002*\"ISBN\" + 0.002*\"Jess\" + 0.002*\"release\" + 0.002*\"Irony\" + 0.002*\"Harrison\" + 0.002*\"Zhenya\" + 0.002*\"DVD\" + 0.002*\"httpwww\" + 0.002*\"title\" + 0.002*\"s\" + 0.002*\"Movie\" + 0.002*\"States\" + 0.001*\"Movies\" + 0.001*\"V\" + 0.001*\"Freed\" + 0.001*\"Times\" + 0.001*\"n\" + 0.001*\"phone\" + 0.001*\"role\" + 0.001*\"house\" + 0.001*\"Horror\" + 0.001*\"story\" + 0.001*\"Mrs\" + 0.001*\"Films\" + 0.001*\"Hussey\" + 0.001*\"С\" + 0.001*\"sorority\" + 0.001*\"de\" + 0.001*\"killer\" + 0.001*\"e\" + 0.001*\"Bluray\" + 0.001*\"ax\" + 0.001*\"IMDb\" + 0.001*\"Years\" + 0.001*\"Midler\" + 0.001*\"Me\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1980:  [(0, '0.012*\"film\" + 0.012*\"Terminator\" + 0.008*\"httpswww\" + 0.005*\"p\" + 0.004*\"T\" + 0.004*\"Film\" + 0.003*\"time\" + 0.003*\"movie\" + 0.003*\"Sarah\" + 0.003*\"Pieces\" + 0.003*\"V\" + 0.002*\"gang\" + 0.002*\"Schwarzenegger\" + 0.002*\"films\" + 0.002*\"W\" + 0.002*\"McGavin\" + 0.002*\"erminator\" + 0.002*\"ISBN\" + 0.002*\"Colors\" + 0.002*\"release\" + 0.002*\"Mohan\" + 0.002*\"Hodges\" + 0.002*\"police\" + 0.002*\"action\" + 0.002*\"Cocoon\" + 0.002*\"S\" + 0.002*\"imes\" + 0.002*\"Police\" + 0.001*\"River\" + 0.001*\"httpwww\" + 0.001*\"Cadillac\" + 0.001*\"Hurd\" + 0.001*\"Box\" + 0.001*\"States\" + 0.001*\"farm\" + 0.001*\"Orion\" + 0.001*\"Review\" + 0.001*\"score\" + 0.001*\"Entertainment\" + 0.001*\"e\" + 0.001*\"Return\" + 0.001*\"Tomatoes\" + 0.001*\"Ashok\" + 0.001*\"office\" + 0.001*\"Films\" + 0.001*\"stars\" + 0.001*\"scenes\" + 0.001*\"soundtrack\" + 0.001*\"script\" + 0.001*\"story\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 1990:  [(0, '0.010*\"film\" + 0.009*\"httpswww\" + 0.004*\"W\" + 0.003*\"Film\" + 0.003*\"httpwww\" + 0.003*\"Shes\" + 0.003*\"s\" + 0.002*\"Foster\" + 0.002*\"Laney\" + 0.002*\"movie\" + 0.002*\"T\" + 0.002*\"States\" + 0.002*\"Zack\" + 0.002*\"films\" + 0.002*\"Awards\" + 0.002*\"time\" + 0.002*\"release\" + 0.002*\"Club\" + 0.002*\"imes\" + 0.002*\"Box\" + 0.002*\"character\" + 0.002*\"f\" + 0.002*\"Entertainment\" + 0.002*\"Davis\" + 0.002*\"Peacemaker\" + 0.002*\"Harrison\" + 0.002*\"Nick\" + 0.001*\"html\" + 0.001*\"Players\" + 0.001*\"Cook\" + 0.001*\"DVD\" + 0.001*\"e\" + 0.001*\"action\" + 0.001*\"US\" + 0.001*\"Andrew\" + 0.001*\"Tommy\" + 0.001*\"office\" + 0.001*\"Jr\" + 0.001*\"scenes\" + 0.001*\"scene\" + 0.001*\"Ebert\" + 0.001*\"Dorothy\" + 0.001*\"St\" + 0.001*\"p\" + 0.001*\"Dr\" + 0.001*\"TK\" + 0.001*\"Roger\" + 0.001*\"Ebony\" + 0.001*\"Surrender\" + 0.001*\"Variety\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2000:  [(0, '0.011*\"Sivaji\" + 0.007*\"film\" + 0.006*\"Vamsi\" + 0.006*\"Emil\" + 0.006*\"Seetha\" + 0.005*\"puppies\" + 0.004*\"httpswww\" + 0.004*\"V\" + 0.004*\"Buddies\" + 0.004*\"Oleg\" + 0.004*\"Minutes\" + 0.003*\"Adam\" + 0.003*\"leaves\" + 0.003*\"life\" + 0.003*\"Anjaneyulu\" + 0.002*\"father\" + 0.002*\"Jean\" + 0.002*\"race\" + 0.002*\"httpwww\" + 0.002*\"time\" + 0.002*\"Merry\" + 0.002*\"Gentleman\" + 0.002*\"S\" + 0.002*\"Ganga\" + 0.002*\"Shasta\" + 0.002*\"e\" + 0.002*\"friend\" + 0.002*\"Geor\" + 0.002*\"parents\" + 0.002*\"f\" + 0.002*\"IMDb\" + 0.002*\"date\" + 0.002*\"marriage\" + 0.002*\"Naveen\" + 0.002*\"Ravi\" + 0.002*\"movie\" + 0.002*\"g\" + 0.002*\"office\" + 0.002*\"Thaman\" + 0.002*\"Language\" + 0.002*\"Entertainment\" + 0.002*\"Chala\" + 0.002*\"links\" + 0.002*\"night\" + 0.002*\"Srikanth\" + 0.002*\"Snow\" + 0.002*\"rating\" + 0.002*\"day\" + 0.002*\"Bagundi\" + 0.002*\"ge\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n",
      "Most used words for the decade 2010:  [(0, '0.014*\"film\" + 0.008*\"httpswww\" + 0.008*\"Film\" + 0.008*\"Network\" + 0.006*\"httpwww\" + 0.004*\"g\" + 0.004*\"Facebook\" + 0.003*\"Super\" + 0.003*\"Upendra\" + 0.003*\"movie\" + 0.002*\"films\" + 0.002*\"Bone\" + 0.002*\"Awards\" + 0.002*\"time\" + 0.002*\"Festival\" + 0.002*\"Winters\" + 0.002*\"Club\" + 0.002*\"W\" + 0.002*\"release\" + 0.002*\"Wayback\" + 0.002*\"V\" + 0.002*\"Socialisme\" + 0.002*\"Machine\" + 0.002*\"Ree\" + 0.002*\"T\" + 0.002*\"story\" + 0.002*\"Joyce\" + 0.001*\"Fincher\" + 0.001*\"Andrew\" + 0.001*\"Box\" + 0.001*\"Guilt\" + 0.001*\"critics\" + 0.001*\"Mark\" + 0.001*\"people\" + 0.001*\"Saverin\" + 0.001*\"s\" + 0.001*\"father\" + 0.001*\"Academy\" + 0.001*\"review\" + 0.001*\"students\" + 0.001*\"f\" + 0.001*\"script\" + 0.001*\"Award\" + 0.001*\"stars\" + 0.001*\"work\" + 0.001*\"screenplay\" + 0.001*\"won\" + 0.001*\"Release\" + 0.001*\"Trip\" + 0.001*\"rating\"')]\n",
      "the type of frequencies  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#initializing decades name\n",
    "decade_name = [1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010]\n",
    "i=0\n",
    "\n",
    "for decade in summary_per_decade :\n",
    "    # Process the text \n",
    "    decade = \" \".join(decade)\n",
    "    splitted_decade=text_split_new(decade)\n",
    "    bag_decade=bag_of_words(splitted_decade)\n",
    "    lda_decade=lda_new(bag_decade)\n",
    "    \n",
    "    # print the topics and their frequencies\n",
    "    print(f\"Most used words for the decade {decade_name[i]}: \",get_topics_new(lda_decade))\n",
    "    print(\"the type of frequencies \", type(lda_decade.get_topics()))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       film  films      p   Film      W     de   ISBN  Razin  Silent  \\\n",
      "1900  0.020  0.007  0.006  0.006  0.006  0.005  0.005  0.005   0.005   \n",
      "1910  0.016  0.005  0.004  0.009  0.000  0.000  0.000  0.000   0.006   \n",
      "1920  0.016  0.003  0.000  0.006  0.004  0.000  0.002  0.000   0.005   \n",
      "1930  0.007  0.000  0.000  0.000  0.005  0.000  0.000  0.000   0.000   \n",
      "1940  0.014  0.005  0.010  0.004  0.002  0.000  0.003  0.000   0.000   \n",
      "1950  0.013  0.002  0.005  0.006  0.003  0.000  0.003  0.000   0.000   \n",
      "1960  0.013  0.003  0.007  0.003  0.003  0.000  0.004  0.000   0.000   \n",
      "1970  0.013  0.003  0.003  0.004  0.004  0.001  0.002  0.000   0.000   \n",
      "1980  0.012  0.002  0.005  0.004  0.002  0.000  0.002  0.000   0.000   \n",
      "1990  0.010  0.002  0.001  0.003  0.004  0.000  0.000  0.000   0.000   \n",
      "2000  0.007  0.000  0.000  0.000  0.000  0.000  0.000  0.000   0.000   \n",
      "2010  0.014  0.002  0.000  0.008  0.002  0.000  0.000  0.000   0.000   \n",
      "\n",
      "      httpswww  ...  people   Mark  Saverin  students  Award   work  rating  \\\n",
      "1900     0.004  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1910     0.006  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1920     0.009  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1930     0.007  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1940     0.006  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1950     0.006  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1960     0.006  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1970     0.006  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1980     0.008  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "1990     0.009  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "2000     0.004  ...   0.000  0.000    0.000     0.000  0.000  0.000   0.000   \n",
      "2010     0.008  ...   0.001  0.001    0.001     0.001  0.001  0.001   0.001   \n",
      "\n",
      "        won   Trip  screenplay  \n",
      "1900  0.000  0.000       0.000  \n",
      "1910  0.000  0.000       0.000  \n",
      "1920  0.000  0.000       0.000  \n",
      "1930  0.000  0.000       0.000  \n",
      "1940  0.000  0.000       0.000  \n",
      "1950  0.000  0.000       0.000  \n",
      "1960  0.000  0.000       0.000  \n",
      "1970  0.000  0.000       0.000  \n",
      "1980  0.000  0.000       0.000  \n",
      "1990  0.000  0.000       0.000  \n",
      "2000  0.000  0.000       0.000  \n",
      "2010  0.001  0.001       0.001  \n",
      "\n",
      "[12 rows x 364 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store word frequencies for each decade\n",
    "word_frequencies_summaries = defaultdict(lambda: defaultdict(int))\n",
    "i=0\n",
    "\n",
    "for decade in summary_per_decade:\n",
    "    # Process the text\n",
    "    decade = \" \".join(decade)\n",
    "    splitted_decade = text_split_new(decade)\n",
    "    bag_decade = bag_of_words(splitted_decade)\n",
    "    lda_decade = lda_new(bag_decade)\n",
    "    \n",
    "    # Get the topics and their frequencies\n",
    "    topics_str = get_topics_new(lda_decade)\n",
    "    # extract() function was written to extract the topics and their frequencies given their type\n",
    "    topics = extract_new(topics_str)\n",
    "    \n",
    "    #print(f\"Most used words for the decade {decade_name}: \", topics)\n",
    "    # Update the word frequencies for each decade\n",
    "    for word, freq in topics:\n",
    "        word_frequencies_summaries[decade_name[i]][word] = freq\n",
    "    i+=1\n",
    "\n",
    "# Convert the word frequencies dictionary to a DataFrame\n",
    "df_word_frequencies = pd.DataFrame(word_frequencies_summaries).fillna(0)\n",
    "\n",
    "# Transpose the DataFrame to have decades as rows and words as columns\n",
    "df_word_frequencies = df_word_frequencies.T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_word_frequencies)\n",
    "\n",
    "#save in csv file\n",
    "df_word_frequencies.to_csv(\"word_frequencies_summaries_wikipedia.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
